.version 2.0
.target sm_10, map_f64_to_f32

// shader: MESA_SHADER_CLOSEST_HIT
// inputs: 0
// outputs: 0
// uniforms: 0
// shared: 0
// decl_function main (0 params)
.entry MESA_SHADER_CLOSEST_HIT_func3_main () {
	.reg  .b32 %ssa_542;

	.reg  .b32 %ssa_539;

	.reg  .b32 %ssa_536;

	.reg  .b32 %ssa_533;

	.reg  .b32 %ssa_529;

	.reg  .b32 %ssa_526;

	.reg  .b32 %ssa_523;

	.reg  .b32 %ssa_520;

	.reg  .s32 %ssa_422;

		.reg  .b32 %ssa_516;

		.reg  .b32 %ssa_513;

		.reg  .b32 %ssa_510;

		.reg  .b32 %ssa_507;

		.reg  .b32 %ssa_503;

		.reg  .b32 %ssa_500;

		.reg  .b32 %ssa_497;

		.reg  .b32 %ssa_494;

		.reg  .s32 %ssa_419;

			.reg  .b32 %ssa_490;

			.reg  .b32 %ssa_487;

			.reg  .b32 %ssa_484;

			.reg  .b32 %ssa_481;

			.reg  .f32 %ssa_477;

			.reg  .f32 %ssa_474;

			.reg  .f32 %ssa_471;

			.reg  .f32 %ssa_468;

			.reg  .s32 %ssa_416;

				.reg  .f32 %ssa_464;

				.reg  .f32 %ssa_461;

				.reg  .f32 %ssa_458;

				.reg  .f32 %ssa_455;

				.reg  .f32 %ssa_382;

				.reg  .s32 %ssa_272;

			.reg  .f32 %ssa_451;

			.reg  .f32 %ssa_448;

			.reg  .f32 %ssa_445;

			.reg  .f32 %ssa_442;

			.reg  .s32 %ssa_203;

		.reg  .f32 %ssa_438;

		.reg  .f32 %ssa_435;

		.reg  .f32 %ssa_432;

		.reg  .f32 %ssa_429;

	.reg .b64 %TextureSamplers;
	rt_alloc_mem %TextureSamplers, 4, 2; // decl_var uniform INTERP_MODE_NONE restrict sampler2D[] TextureSamplers (~0, 0, 8)
	.reg .b64 %Ray;
	rt_alloc_mem %Ray, 36, 32; // decl_var shader_call_data INTERP_MODE_NONE RayPayload Ray


	.reg .u64 %temp_u64;
	.reg .u32 %temp_u32;
	.reg .f32 %temp_f32;
	.reg .pred %temp_pred;
	.reg .f32 %const1_f32;
	mov.f32 %const1_f32, 0F3f800000;

	.reg .f32 %const0_f32;
	mov.f32 %const0_f32, 0F00000000;

	.reg .u32 %const0_u32;
	mov.u32 %const0_u32, 0;

	.reg .u16 %const1_u16;
	mov.u16 %const1_u16, 1;

	// start_block block_0:
	// preds: 
	.reg .u32 %ssa_0;
	load_ray_instance_custom_index %ssa_0;	// vec1 32 ssa_0 = intrinsic load_ray_instance_custom_index () ()

	.reg .f32 %ssa_1;
	mov.f32 %ssa_1, 0F00000000; // vec1 32 ssa_1 = load_const (0x00000000 /* 0.000000 */)
	.reg .b32 %ssa_1_bits;
	mov.b32 %ssa_1_bits, 0F00000000;

	.reg .b64 %ssa_2;
	load_vulkan_descriptor %ssa_2, 0, 7, 7; // vec2 32 ssa_2 = intrinsic vulkan_resource_index (%ssa_1) (0, 7, 7) /* desc_set=0 */ /* binding=7 */ /* desc_type=SSBO */

	.reg .b64 %ssa_3;
	mov.b64 %ssa_3, %ssa_2; // vec2 32 ssa_3 = intrinsic load_vulkan_descriptor (%ssa_2) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_4;
	mov.b64 %ssa_4, %ssa_3; // vec2 32 ssa_4 = deref_cast (OffsetArray *)ssa_3 (ssbo OffsetArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_5;
	add.u64 %ssa_5, %ssa_4, 0; // vec2 32 ssa_5 = deref_struct &ssa_4->Offsets (ssbo uvec2[]) /* &((OffsetArray *)ssa_3)->Offsets */

	.reg .b64 %ssa_6;
	.reg .u32 %ssa_6_array_index_32;
	.reg .u64 %ssa_6_array_index_64;
	mov.u32 %ssa_6_array_index_32, %ssa_0;
	mul.wide.u32 %ssa_6_array_index_64, %ssa_6_array_index_32, 8;
	add.u64 %ssa_6, %ssa_5, %ssa_6_array_index_64; // vec2 32 ssa_6 = deref_array &(*ssa_5)[ssa_0] (ssbo uvec2) /* &((OffsetArray *)ssa_3)->Offsets[ssa_0] */

	.reg .u32 %ssa_7_0;
	.reg .u32 %ssa_7_1;
	ld.global.u32 %ssa_7_0, [%ssa_6 + 0];
	ld.global.u32 %ssa_7_1, [%ssa_6 + 4];
// vec2 32 ssa_7 = intrinsic load_deref (%ssa_6) (16) /* access=16 */


	.reg .u32 %ssa_8;
	mov.u32 %ssa_8, %ssa_7_0; // vec1 32 ssa_8 = mov ssa_7.x

	.reg .b64 %ssa_9;
	load_vulkan_descriptor %ssa_9, 0, 5, 7; // vec2 32 ssa_9 = intrinsic vulkan_resource_index (%ssa_1) (0, 5, 7) /* desc_set=0 */ /* binding=5 */ /* desc_type=SSBO */

	.reg .b64 %ssa_10;
	mov.b64 %ssa_10, %ssa_9; // vec2 32 ssa_10 = intrinsic load_vulkan_descriptor (%ssa_9) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_11;
	mov.b64 %ssa_11, %ssa_10; // vec2 32 ssa_11 = deref_cast (IndexArray *)ssa_10 (ssbo IndexArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_12;
	add.u64 %ssa_12, %ssa_11, 0; // vec2 32 ssa_12 = deref_struct &ssa_11->Indices (ssbo uint[]) /* &((IndexArray *)ssa_10)->Indices */

	.reg .b64 %ssa_13;
	.reg .u32 %ssa_13_array_index_32;
	.reg .u64 %ssa_13_array_index_64;
	mov.u32 %ssa_13_array_index_32, %ssa_8;
	mul.wide.u32 %ssa_13_array_index_64, %ssa_13_array_index_32, 4;
	add.u64 %ssa_13, %ssa_12, %ssa_13_array_index_64; // vec2 32 ssa_13 = deref_array &(*ssa_12)[ssa_8] (ssbo uint) /* &((IndexArray *)ssa_10)->Indices[ssa_8] */

	.reg  .u32 %ssa_14;
	ld.global.u32 %ssa_14, [%ssa_13]; // vec1 32 ssa_14 = intrinsic load_deref (%ssa_13) (16) /* access=16 */

	.reg .s32 %ssa_15;
	add.s32 %ssa_15, %ssa_7_1, %ssa_14; // vec1 32 ssa_15 = iadd ssa_7.y, ssa_14

	.reg .f32 %ssa_16;
	mov.f32 %ssa_16, 0F00000008; // vec1 32 ssa_16 = load_const (0x00000008 /* 0.000000 */)
	.reg .b32 %ssa_16_bits;
	mov.b32 %ssa_16_bits, 0F00000008;

	.reg .f32 %ssa_17;
	mov.f32 %ssa_17, 0F00000009; // vec1 32 ssa_17 = load_const (0x00000009 /* 0.000000 */)
	.reg .b32 %ssa_17_bits;
	mov.b32 %ssa_17_bits, 0F00000009;

	.reg .s32 %ssa_18;
	mul.lo.s32 %ssa_18, %ssa_15, %ssa_17_bits; // vec1 32 ssa_18 = imul ssa_15, ssa_17

	.reg .s32 %ssa_19;
	add.s32 %ssa_19, %ssa_18, %ssa_16_bits; // vec1 32 ssa_19 = iadd ssa_18, ssa_16

	.reg .b64 %ssa_20;
	load_vulkan_descriptor %ssa_20, 0, 4, 7; // vec2 32 ssa_20 = intrinsic vulkan_resource_index (%ssa_1) (0, 4, 7) /* desc_set=0 */ /* binding=4 */ /* desc_type=SSBO */

	.reg .b64 %ssa_21;
	mov.b64 %ssa_21, %ssa_20; // vec2 32 ssa_21 = intrinsic load_vulkan_descriptor (%ssa_20) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_22;
	mov.b64 %ssa_22, %ssa_21; // vec2 32 ssa_22 = deref_cast (VertexArray *)ssa_21 (ssbo VertexArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_23;
	add.u64 %ssa_23, %ssa_22, 0; // vec2 32 ssa_23 = deref_struct &ssa_22->Vertices (ssbo float[]) /* &((VertexArray *)ssa_21)->Vertices */

	.reg .b64 %ssa_24;
	.reg .u32 %ssa_24_array_index_32;
	.reg .u64 %ssa_24_array_index_64;
	cvt.u32.s32 %ssa_24_array_index_32, %ssa_19;
	mul.wide.u32 %ssa_24_array_index_64, %ssa_24_array_index_32, 4;
	add.u64 %ssa_24, %ssa_23, %ssa_24_array_index_64; // vec2 32 ssa_24 = deref_array &(*ssa_23)[ssa_19] (ssbo float) /* &((VertexArray *)ssa_21)->Vertices[ssa_19] */

	.reg  .f32 %ssa_25;
	ld.global.f32 %ssa_25, [%ssa_24]; // vec1 32 ssa_25 = intrinsic load_deref (%ssa_24) (16) /* access=16 */

	.reg .b64 %ssa_26;
	load_vulkan_descriptor %ssa_26, 0, 6, 7; // vec2 32 ssa_26 = intrinsic vulkan_resource_index (%ssa_1) (0, 6, 7) /* desc_set=0 */ /* binding=6 */ /* desc_type=SSBO */

	.reg .b64 %ssa_27;
	mov.b64 %ssa_27, %ssa_26; // vec2 32 ssa_27 = intrinsic load_vulkan_descriptor (%ssa_26) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_28;
	mov.b64 %ssa_28, %ssa_27; // vec2 32 ssa_28 = deref_cast (MaterialArray *)ssa_27 (ssbo MaterialArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_29;
	add.u64 %ssa_29, %ssa_28, 0; // vec2 32 ssa_29 = deref_struct &ssa_28->Materials (ssbo Material[]) /* &((MaterialArray *)ssa_27)->Materials */

	.reg .b64 %ssa_30;
	.reg .u32 %ssa_30_array_index_32;
	.reg .u64 %ssa_30_array_index_64;
	mov.b32 %ssa_30_array_index_32, %ssa_25;
	mul.wide.u32 %ssa_30_array_index_64, %ssa_30_array_index_32, 32;
	add.u64 %ssa_30, %ssa_29, %ssa_30_array_index_64; // vec2 32 ssa_30 = deref_array &(*ssa_29)[ssa_25] (ssbo Material) /* &((MaterialArray *)ssa_27)->Materials[ssa_25] */

	.reg .b64 %ssa_31;
	add.u64 %ssa_31, %ssa_30, 0; // vec2 32 ssa_31 = deref_struct &ssa_30->Diffuse (ssbo vec4) /* &((MaterialArray *)ssa_27)->Materials[ssa_25].Diffuse */

	.reg .f32 %ssa_32_0;
	.reg .f32 %ssa_32_1;
	.reg .f32 %ssa_32_2;
	.reg .f32 %ssa_32_3;
	ld.global.f32 %ssa_32_0, [%ssa_31 + 0];
	ld.global.f32 %ssa_32_1, [%ssa_31 + 4];
	ld.global.f32 %ssa_32_2, [%ssa_31 + 8];
	ld.global.f32 %ssa_32_3, [%ssa_31 + 12];
// vec4 32 ssa_32 = intrinsic load_deref (%ssa_31) (16) /* access=16 */


	.reg .b64 %ssa_33;
	add.u64 %ssa_33, %ssa_30, 16; // vec2 32 ssa_33 = deref_struct &ssa_30->DiffuseTextureId (ssbo int) /* &((MaterialArray *)ssa_27)->Materials[ssa_25].DiffuseTextureId */

	.reg  .s32 %ssa_34;
	ld.global.s32 %ssa_34, [%ssa_33]; // vec1 32 ssa_34 = intrinsic load_deref (%ssa_33) (16) /* access=16 */

	.reg .b64 %ssa_35;
	add.u64 %ssa_35, %ssa_30, 20; // vec2 32 ssa_35 = deref_struct &ssa_30->Fuzziness (ssbo float) /* &((MaterialArray *)ssa_27)->Materials[ssa_25].Fuzziness */

	.reg  .f32 %ssa_36;
	ld.global.f32 %ssa_36, [%ssa_35]; // vec1 32 ssa_36 = intrinsic load_deref (%ssa_35) (16) /* access=16 */

	.reg .b64 %ssa_37;
	add.u64 %ssa_37, %ssa_30, 24; // vec2 32 ssa_37 = deref_struct &ssa_30->RefractionIndex (ssbo float) /* &((MaterialArray *)ssa_27)->Materials[ssa_25].RefractionIndex */

	.reg  .f32 %ssa_38;
	ld.global.f32 %ssa_38, [%ssa_37]; // vec1 32 ssa_38 = intrinsic load_deref (%ssa_37) (16) /* access=16 */

	.reg .b64 %ssa_39;
	add.u64 %ssa_39, %ssa_30, 28; // vec2 32 ssa_39 = deref_struct &ssa_30->MaterialModel (ssbo uint) /* &((MaterialArray *)ssa_27)->Materials[ssa_25].MaterialModel */

	.reg  .u32 %ssa_40;
	ld.global.u32 %ssa_40, [%ssa_39]; // vec1 32 ssa_40 = intrinsic load_deref (%ssa_39) (16) /* access=16 */

	.reg .b64 %ssa_41;
	load_vulkan_descriptor %ssa_41, 0, 9, 7; // vec2 32 ssa_41 = intrinsic vulkan_resource_index (%ssa_1) (0, 9, 7) /* desc_set=0 */ /* binding=9 */ /* desc_type=SSBO */

	.reg .b64 %ssa_42;
	mov.b64 %ssa_42, %ssa_41; // vec2 32 ssa_42 = intrinsic load_vulkan_descriptor (%ssa_41) (7) /* desc_type=SSBO */

	.reg .b64 %ssa_43;
	mov.b64 %ssa_43, %ssa_42; // vec2 32 ssa_43 = deref_cast (SphereArray *)ssa_42 (ssbo SphereArray)  /* ptr_stride=0, align_mul=0, align_offset=0 */

	.reg .b64 %ssa_44;
	add.u64 %ssa_44, %ssa_43, 0; // vec2 32 ssa_44 = deref_struct &ssa_43->Spheres (ssbo vec4[]) /* &((SphereArray *)ssa_42)->Spheres */

	.reg .b64 %ssa_45;
	.reg .u32 %ssa_45_array_index_32;
	.reg .u64 %ssa_45_array_index_64;
	mov.u32 %ssa_45_array_index_32, %ssa_0;
	mul.wide.u32 %ssa_45_array_index_64, %ssa_45_array_index_32, 16;
	add.u64 %ssa_45, %ssa_44, %ssa_45_array_index_64; // vec2 32 ssa_45 = deref_array &(*ssa_44)[ssa_0] (ssbo vec4) /* &((SphereArray *)ssa_42)->Spheres[ssa_0] */

	.reg .f32 %ssa_46_0;
	.reg .f32 %ssa_46_1;
	.reg .f32 %ssa_46_2;
	.reg .f32 %ssa_46_3;
	ld.global.f32 %ssa_46_0, [%ssa_45 + 0];
	ld.global.f32 %ssa_46_1, [%ssa_45 + 4];
	ld.global.f32 %ssa_46_2, [%ssa_45 + 8];
	ld.global.f32 %ssa_46_3, [%ssa_45 + 12];
// vec4 32 ssa_46 = intrinsic load_deref (%ssa_45) (16) /* access=16 */


	.reg .f32 %ssa_47_0;
	.reg .f32 %ssa_47_1;
	.reg .f32 %ssa_47_2;
	.reg .f32 %ssa_47_3;
	.reg .b64 %ssa_47_address;
	load_ray_world_origin %ssa_47_address; // vec3 32 ssa_47 = intrinsic load_ray_world_origin () ()
	ld.global.f32 %ssa_47_0, [%ssa_47_address + 0];
	ld.global.f32 %ssa_47_1, [%ssa_47_address + 4];
	ld.global.f32 %ssa_47_2, [%ssa_47_address + 8];
	ld.global.f32 %ssa_47_3, [%ssa_47_address + 12];

	.reg .f32 %ssa_48;
	load_ray_t_max %ssa_48;	// vec1 32 ssa_48 = intrinsic load_ray_t_max () ()

	.reg .f32 %ssa_49_0;
	.reg .f32 %ssa_49_1;
	.reg .f32 %ssa_49_2;
	.reg .f32 %ssa_49_3;
	.reg .b64 %ssa_49_address;
	load_ray_world_direction %ssa_49_address; // vec3 32 ssa_49 = intrinsic load_ray_world_direction () ()
	ld.global.f32 %ssa_49_0, [%ssa_49_address + 0];
	ld.global.f32 %ssa_49_1, [%ssa_49_address + 4];
	ld.global.f32 %ssa_49_2, [%ssa_49_address + 8];
	ld.global.f32 %ssa_49_3, [%ssa_49_address + 12];

	.reg .f32 %ssa_50;
	mul.f32 %ssa_50, %ssa_49_0, %ssa_48; // vec1 32 ssa_50 = fmul ssa_49.x, ssa_48

	.reg .f32 %ssa_51;
	mul.f32 %ssa_51, %ssa_49_1, %ssa_48; // vec1 32 ssa_51 = fmul ssa_49.y, ssa_48

	.reg .f32 %ssa_52;
	mul.f32 %ssa_52, %ssa_49_2, %ssa_48; // vec1 32 ssa_52 = fmul ssa_49.z, ssa_48

	.reg .f32 %ssa_53;
	add.f32 %ssa_53, %ssa_47_0, %ssa_50; // vec1 32 ssa_53 = fadd ssa_47.x, ssa_50

	.reg .f32 %ssa_54;
	add.f32 %ssa_54, %ssa_47_1, %ssa_51; // vec1 32 ssa_54 = fadd ssa_47.y, ssa_51

	.reg .f32 %ssa_55;
	add.f32 %ssa_55, %ssa_47_2, %ssa_52; // vec1 32 ssa_55 = fadd ssa_47.z, ssa_52

	.reg .f32 %ssa_56;
	neg.f32 %ssa_56, %ssa_46_0; // vec1 32 ssa_56 = fneg ssa_46.x

	.reg .f32 %ssa_57;
	neg.f32 %ssa_57, %ssa_46_1; // vec1 32 ssa_57 = fneg ssa_46.y

	.reg .f32 %ssa_58;
	neg.f32 %ssa_58, %ssa_46_2; // vec1 32 ssa_58 = fneg ssa_46.z

	.reg .f32 %ssa_59;
	add.f32 %ssa_59, %ssa_53, %ssa_56;	// vec1 32 ssa_59 = fadd ssa_53, ssa_56

	.reg .f32 %ssa_60;
	add.f32 %ssa_60, %ssa_54, %ssa_57;	// vec1 32 ssa_60 = fadd ssa_54, ssa_57

	.reg .f32 %ssa_61;
	add.f32 %ssa_61, %ssa_55, %ssa_58;	// vec1 32 ssa_61 = fadd ssa_55, ssa_58

	.reg .f32 %ssa_62;
	rcp.approx.f32 %ssa_62, %ssa_46_3; // vec1 32 ssa_62 = frcp ssa_46.w

	.reg .f32 %ssa_63;
	mul.f32 %ssa_63, %ssa_59, %ssa_62;	// vec1 32 ssa_63 = fmul ssa_59, ssa_62

	.reg .f32 %ssa_64;
	mul.f32 %ssa_64, %ssa_60, %ssa_62;	// vec1 32 ssa_64 = fmul ssa_60, ssa_62

	.reg .f32 %ssa_65;
	mul.f32 %ssa_65, %ssa_61, %ssa_62;	// vec1 32 ssa_65 = fmul ssa_61, ssa_62

	.reg .f32 %ssa_66;
	mov.f32 %ssa_66, 0F3f800000; // vec1 32 ssa_66 = load_const (0x3f800000 /* 1.000000 */)
	.reg .b32 %ssa_66_bits;
	mov.b32 %ssa_66_bits, 0F3f800000;

	.reg .f32 %ssa_67;
	mov.f32 %ssa_67, 0F3fc90fdb; // vec1 32 ssa_67 = load_const (0x3fc90fdb /* 1.570796 */)
	.reg .b32 %ssa_67_bits;
	mov.b32 %ssa_67_bits, 0F3fc90fdb;

	.reg .f32 %ssa_68;
	mov.f32 %ssa_68, 0F40490fdb; // vec1 32 ssa_68 = load_const (0x40490fdb /* 3.141593 */)
	.reg .b32 %ssa_68_bits;
	mov.b32 %ssa_68_bits, 0F40490fdb;

	.reg .pred %ssa_69;
	setp.ge.f32 %ssa_69, %ssa_1, %ssa_65;	// vec1  1 ssa_69 = fge ssa_1, ssa_65

	.reg .f32 %ssa_70;
	abs.f32 %ssa_70, %ssa_65;	// vec1 32 ssa_70 = fabs ssa_65

	.reg  .f32 %ssa_71;
	selp.f32 %ssa_71, %ssa_70, %ssa_63, %ssa_69; // vec1 32 ssa_71 = bcsel ssa_69, ssa_70, ssa_63

	.reg  .f32 %ssa_72;
	selp.f32 %ssa_72, %ssa_63, %ssa_70, %ssa_69; // vec1 32 ssa_72 = bcsel ssa_69, ssa_63, ssa_70

	.reg .f32 %ssa_73;
	mov.f32 %ssa_73, 0F5d5e0b6b; // vec1 32 ssa_73 = load_const (0x5d5e0b6b /* 999999984306749440.000000 */)
	.reg .b32 %ssa_73_bits;
	mov.b32 %ssa_73_bits, 0F5d5e0b6b;

	.reg .f32 %ssa_74;
	mov.f32 %ssa_74, 0F3e800000; // vec1 32 ssa_74 = load_const (0x3e800000 /* 0.250000 */)
	.reg .b32 %ssa_74_bits;
	mov.b32 %ssa_74_bits, 0F3e800000;

	.reg .f32 %ssa_75;
	abs.f32 %ssa_75, %ssa_72;	// vec1 32 ssa_75 = fabs ssa_72

	.reg .pred %ssa_76;
	setp.ge.f32 %ssa_76, %ssa_75, %ssa_73;	// vec1  1 ssa_76 = fge ssa_75, ssa_73

	.reg  .f32 %ssa_77;
	selp.f32 %ssa_77, %ssa_74_bits, %ssa_66_bits, %ssa_76; // vec1 32 ssa_77 = bcsel ssa_76, ssa_74, ssa_66

	.reg .f32 %ssa_78;
	mul.f32 %ssa_78, %ssa_72, %ssa_77;	// vec1 32 ssa_78 = fmul ssa_72, ssa_77

	.reg .f32 %ssa_79;
	rcp.approx.f32 %ssa_79, %ssa_78;	// vec1 32 ssa_79 = frcp ssa_78

	.reg .f32 %ssa_80;
	mul.f32 %ssa_80, %ssa_71, %ssa_77;	// vec1 32 ssa_80 = fmul ssa_71, ssa_77

	.reg .f32 %ssa_81;
	mul.f32 %ssa_81, %ssa_80, %ssa_79;	// vec1 32 ssa_81 = fmul ssa_80, ssa_79

	.reg .f32 %ssa_82;
	abs.f32 %ssa_82, %ssa_81;	// vec1 32 ssa_82 = fabs ssa_81

	.reg .f32 %ssa_83;
	abs.f32 %ssa_83, %ssa_63;	// vec1 32 ssa_83 = fabs ssa_63

	.reg .pred %ssa_84;
	setp.eq.f32 %ssa_84, %ssa_70, %ssa_83;	// vec1  1 ssa_84 = feq ssa_70, ssa_83

	.reg  .f32 %ssa_85;
	selp.f32 %ssa_85, %ssa_66_bits, %ssa_82, %ssa_84; // vec1 32 ssa_85 = bcsel ssa_84, ssa_66, ssa_82

	.reg .f32 %ssa_86;
	max.f32 %ssa_86, %ssa_85, %ssa_66;	// vec1 32 ssa_86 = fmax ssa_85, ssa_66

	.reg .f32 %ssa_87;
	min.f32 %ssa_87, %ssa_85, %ssa_66;	// vec1 32 ssa_87 = fmin ssa_85, ssa_66

	.reg .f32 %ssa_88;
	rcp.approx.f32 %ssa_88, %ssa_86;	// vec1 32 ssa_88 = frcp ssa_86

	.reg .f32 %ssa_89;
	mul.f32 %ssa_89, %ssa_87, %ssa_88;	// vec1 32 ssa_89 = fmul ssa_87, ssa_88

	.reg .f32 %ssa_90;
	mul.f32 %ssa_90, %ssa_89, %ssa_89;	// vec1 32 ssa_90 = fmul ssa_89, ssa_89

	.reg .f32 %ssa_91;
	mul.f32 %ssa_91, %ssa_90, %ssa_89;	// vec1 32 ssa_91 = fmul ssa_90, ssa_89

	.reg .f32 %ssa_92;
	mul.f32 %ssa_92, %ssa_91, %ssa_90;	// vec1 32 ssa_92 = fmul ssa_91, ssa_90

	.reg .f32 %ssa_93;
	mul.f32 %ssa_93, %ssa_92, %ssa_90;	// vec1 32 ssa_93 = fmul ssa_92, ssa_90

	.reg .f32 %ssa_94;
	mul.f32 %ssa_94, %ssa_93, %ssa_90;	// vec1 32 ssa_94 = fmul ssa_93, ssa_90

	.reg .f32 %ssa_95;
	mov.f32 %ssa_95, 0F3f7ffea5; // vec1 32 ssa_95 = load_const (0x3f7ffea5 /* 0.999979 */)
	.reg .b32 %ssa_95_bits;
	mov.b32 %ssa_95_bits, 0F3f7ffea5;

	.reg .f32 %ssa_96;
	mul.f32 %ssa_96, %ssa_89, %ssa_95;	// vec1 32 ssa_96 = fmul ssa_89, ssa_95

	.reg .f32 %ssa_97;
	mov.f32 %ssa_97, 0Fbeaa5476; // vec1 32 ssa_97 = load_const (0xbeaa5476 /* -0.332676 */)
	.reg .b32 %ssa_97_bits;
	mov.b32 %ssa_97_bits, 0Fbeaa5476;

	.reg .f32 %ssa_98;
	mul.f32 %ssa_98, %ssa_91, %ssa_97;	// vec1 32 ssa_98 = fmul ssa_91, ssa_97

	.reg .f32 %ssa_99;
	mov.f32 %ssa_99, 0F3e468bc1; // vec1 32 ssa_99 = load_const (0x3e468bc1 /* 0.193892 */)
	.reg .b32 %ssa_99_bits;
	mov.b32 %ssa_99_bits, 0F3e468bc1;

	.reg .f32 %ssa_100;
	mul.f32 %ssa_100, %ssa_92, %ssa_99;	// vec1 32 ssa_100 = fmul ssa_92, ssa_99

	.reg .f32 %ssa_101;
	mov.f32 %ssa_101, 0Fbdf0555d; // vec1 32 ssa_101 = load_const (0xbdf0555d /* -0.117350 */)
	.reg .b32 %ssa_101_bits;
	mov.b32 %ssa_101_bits, 0Fbdf0555d;

	.reg .f32 %ssa_102;
	mul.f32 %ssa_102, %ssa_93, %ssa_101;	// vec1 32 ssa_102 = fmul ssa_93, ssa_101

	.reg .f32 %ssa_103;
	mov.f32 %ssa_103, 0F3d5be101; // vec1 32 ssa_103 = load_const (0x3d5be101 /* 0.053681 */)
	.reg .b32 %ssa_103_bits;
	mov.b32 %ssa_103_bits, 0F3d5be101;

	.reg .f32 %ssa_104;
	mul.f32 %ssa_104, %ssa_94, %ssa_103;	// vec1 32 ssa_104 = fmul ssa_94, ssa_103

	.reg .f32 %ssa_105;
	mov.f32 %ssa_105, 0Fbc46c6a5; // vec1 32 ssa_105 = load_const (0xbc46c6a5 /* -0.012132 */)
	.reg .b32 %ssa_105_bits;
	mov.b32 %ssa_105_bits, 0Fbc46c6a5;

	.reg .f32 %ssa_106;
	mul.f32 %ssa_106, %ssa_94, %ssa_105;	// vec1 32 ssa_106 = fmul ssa_94, ssa_105

	.reg .f32 %ssa_107;
	mul.f32 %ssa_107, %ssa_106, %ssa_90;	// vec1 32 ssa_107 = fmul ssa_106, ssa_90

	.reg .f32 %ssa_108;
	add.f32 %ssa_108, %ssa_96, %ssa_98;	// vec1 32 ssa_108 = fadd ssa_96, ssa_98

	.reg .f32 %ssa_109;
	add.f32 %ssa_109, %ssa_108, %ssa_100;	// vec1 32 ssa_109 = fadd ssa_108, ssa_100

	.reg .f32 %ssa_110;
	add.f32 %ssa_110, %ssa_109, %ssa_102;	// vec1 32 ssa_110 = fadd ssa_109, ssa_102

	.reg .f32 %ssa_111;
	add.f32 %ssa_111, %ssa_110, %ssa_104;	// vec1 32 ssa_111 = fadd ssa_110, ssa_104

	.reg .f32 %ssa_112;
	add.f32 %ssa_112, %ssa_111, %ssa_107;	// vec1 32 ssa_112 = fadd ssa_111, ssa_107

	.reg .f32 %ssa_113;
	mov.f32 %ssa_113, 0Fc0000000; // vec1 32 ssa_113 = load_const (0xc0000000 /* -2.000000 */)
	.reg .b32 %ssa_113_bits;
	mov.b32 %ssa_113_bits, 0Fc0000000;

	.reg .f32 %ssa_114;
	mul.f32 %ssa_114, %ssa_112, %ssa_113;	// vec1 32 ssa_114 = fmul ssa_112, ssa_113

	.reg .f32 %ssa_115;
	add.f32 %ssa_115, %ssa_114, %ssa_67;	// vec1 32 ssa_115 = fadd ssa_114, ssa_67

	.reg .pred %ssa_116;
	setp.lt.f32 %ssa_116, %ssa_66, %ssa_85;	// vec1  1 ssa_116 = flt ssa_66, ssa_85

	.reg .f32 %ssa_117;
	selp.f32 %ssa_117, 0F3f800000, 0F00000000, %ssa_116; // vec1 32 ssa_117 = b2f32 ssa_116

	.reg .f32 %ssa_118;
	mul.f32 %ssa_118, %ssa_117, %ssa_115;	// vec1 32 ssa_118 = fmul ssa_117, ssa_115

	.reg .f32 %ssa_119;
	add.f32 %ssa_119, %ssa_118, %ssa_112;	// vec1 32 ssa_119 = fadd ssa_118, ssa_112

	.reg .f32 %ssa_120;
	mov.f32 %ssa_120, 0F3f800000;
	copysignf %ssa_120, %ssa_85; // vec1 32 ssa_120 = fsign ssa_85

	.reg .f32 %ssa_121;
	mul.f32 %ssa_121, %ssa_119, %ssa_120;	// vec1 32 ssa_121 = fmul ssa_119, ssa_120

	.reg .f32 %ssa_122;
	selp.f32 %ssa_122, 0F3f800000, 0F00000000, %ssa_69; // vec1 32 ssa_122 = b2f32 ssa_69

	.reg .f32 %ssa_123;
	mul.f32 %ssa_123, %ssa_122, %ssa_67;	// vec1 32 ssa_123 = fmul ssa_122, ssa_67

	.reg .f32 %ssa_124;
	add.f32 %ssa_124, %ssa_123, %ssa_121;	// vec1 32 ssa_124 = fadd ssa_123, ssa_121

	.reg .f32 %ssa_125;
	neg.f32 %ssa_125, %ssa_124;	// vec1 32 ssa_125 = fneg ssa_124

	.reg .f32 %ssa_126;
	min.f32 %ssa_126, %ssa_63, %ssa_79;	// vec1 32 ssa_126 = fmin ssa_63, ssa_79

	.reg .pred %ssa_127;
	setp.lt.f32 %ssa_127, %ssa_126, %ssa_1;	// vec1  1 ssa_127 = flt ssa_126, ssa_1

	.reg  .f32 %ssa_128;
	selp.f32 %ssa_128, %ssa_125, %ssa_124, %ssa_127; // vec1 32 ssa_128 = bcsel ssa_127, ssa_125, ssa_124

	.reg .f32 %ssa_129;
	mov.f32 %ssa_129, 0F3f000000; // vec1 32 ssa_129 = load_const (0x3f000000 /* 0.500000 */)
	.reg .b32 %ssa_129_bits;
	mov.b32 %ssa_129_bits, 0F3f000000;

	.reg .f32 %ssa_130;
	abs.f32 %ssa_130, %ssa_64;	// vec1 32 ssa_130 = fabs ssa_64

	.reg .f32 %ssa_131;
	mov.f32 %ssa_131, 0F3db149e5; // vec1 32 ssa_131 = load_const (0x3db149e5 /* 0.086567 */)
	.reg .b32 %ssa_131_bits;
	mov.b32 %ssa_131_bits, 0F3db149e5;

	.reg .f32 %ssa_132;
	mov.f32 %ssa_132, 0Fbcfe31af; // vec1 32 ssa_132 = load_const (0xbcfe31af /* -0.031030 */)
	.reg .b32 %ssa_132_bits;
	mov.b32 %ssa_132_bits, 0Fbcfe31af;

	.reg .f32 %ssa_133;
	mul.f32 %ssa_133, %ssa_130, %ssa_132;	// vec1 32 ssa_133 = fmul ssa_130, ssa_132

	.reg .f32 %ssa_134;
	add.f32 %ssa_134, %ssa_133, %ssa_131;	// vec1 32 ssa_134 = fadd ssa_133, ssa_131

	.reg .f32 %ssa_135;
	mov.f32 %ssa_135, 0Fbe5bc094; // vec1 32 ssa_135 = load_const (0xbe5bc094 /* -0.214602 */)
	.reg .b32 %ssa_135_bits;
	mov.b32 %ssa_135_bits, 0Fbe5bc094;

	.reg .f32 %ssa_136;
	mul.f32 %ssa_136, %ssa_130, %ssa_134;	// vec1 32 ssa_136 = fmul ssa_130, ssa_134

	.reg .f32 %ssa_137;
	add.f32 %ssa_137, %ssa_136, %ssa_135;	// vec1 32 ssa_137 = fadd ssa_136, ssa_135

	.reg .f32 %ssa_138;
	mul.f32 %ssa_138, %ssa_130, %ssa_137;	// vec1 32 ssa_138 = fmul ssa_130, ssa_137

	.reg .f32 %ssa_139;
	add.f32 %ssa_139, %ssa_138, %ssa_67;	// vec1 32 ssa_139 = fadd ssa_138, ssa_67

	.reg .f32 %ssa_140;
	neg.f32 %ssa_140, %ssa_130;	// vec1 32 ssa_140 = fneg ssa_130

	.reg .f32 %ssa_141;
	add.f32 %ssa_141, %ssa_66, %ssa_140;	// vec1 32 ssa_141 = fadd ssa_66, ssa_140

	.reg .f32 %ssa_142;
	sqrt.approx.f32 %ssa_142, %ssa_141;	// vec1 32 ssa_142 = fsqrt ssa_141

	.reg .f32 %ssa_143;
	mul.f32 %ssa_143, %ssa_142, %ssa_139;	// vec1 32 ssa_143 = fmul ssa_142, ssa_139

	.reg .f32 %ssa_144;
	neg.f32 %ssa_144, %ssa_143;	// vec1 32 ssa_144 = fneg ssa_143

	.reg .f32 %ssa_145;
	add.f32 %ssa_145, %ssa_144, %ssa_67;	// vec1 32 ssa_145 = fadd ssa_144, ssa_67

	.reg .f32 %ssa_146;
	mov.f32 %ssa_146, 0F3f800000;
	copysignf %ssa_146, %ssa_64; // vec1 32 ssa_146 = fsign ssa_64

	.reg .f32 %ssa_147;
	mul.f32 %ssa_147, %ssa_146, %ssa_145;	// vec1 32 ssa_147 = fmul ssa_146, ssa_145

	.reg .f32 %ssa_148;
	mul.f32 %ssa_148, %ssa_64, %ssa_64;	// vec1 32 ssa_148 = fmul ssa_64, ssa_64

	.reg .f32 %ssa_149;
	mov.f32 %ssa_149, 0Fbd2f13ba; // vec1 32 ssa_149 = load_const (0xbd2f13ba /* -0.042743 */)
	.reg .b32 %ssa_149_bits;
	mov.b32 %ssa_149_bits, 0Fbd2f13ba;

	.reg .f32 %ssa_150;
	mov.f32 %ssa_150, 0Fbc0dd36b; // vec1 32 ssa_150 = load_const (0xbc0dd36b /* -0.008656 */)
	.reg .b32 %ssa_150_bits;
	mov.b32 %ssa_150_bits, 0Fbc0dd36b;

	.reg .f32 %ssa_151;
	mul.f32 %ssa_151, %ssa_148, %ssa_150;	// vec1 32 ssa_151 = fmul ssa_148, ssa_150

	.reg .f32 %ssa_152;
	add.f32 %ssa_152, %ssa_151, %ssa_149;	// vec1 32 ssa_152 = fadd ssa_151, ssa_149

	.reg .f32 %ssa_153;
	mov.f32 %ssa_153, 0F3e2aaa75; // vec1 32 ssa_153 = load_const (0x3e2aaa75 /* 0.166666 */)
	.reg .b32 %ssa_153_bits;
	mov.b32 %ssa_153_bits, 0F3e2aaa75;

	.reg .f32 %ssa_154;
	mul.f32 %ssa_154, %ssa_148, %ssa_152;	// vec1 32 ssa_154 = fmul ssa_148, ssa_152

	.reg .f32 %ssa_155;
	add.f32 %ssa_155, %ssa_154, %ssa_153;	// vec1 32 ssa_155 = fadd ssa_154, ssa_153

	.reg .f32 %ssa_156;
	mul.f32 %ssa_156, %ssa_148, %ssa_155;	// vec1 32 ssa_156 = fmul ssa_148, ssa_155

	.reg .f32 %ssa_157;
	mov.f32 %ssa_157, 0Fbf34e5ae; // vec1 32 ssa_157 = load_const (0xbf34e5ae /* -0.706630 */)
	.reg .b32 %ssa_157_bits;
	mov.b32 %ssa_157_bits, 0Fbf34e5ae;

	.reg .f32 %ssa_158;
	mul.f32 %ssa_158, %ssa_148, %ssa_157;	// vec1 32 ssa_158 = fmul ssa_148, ssa_157

	.reg .f32 %ssa_159;
	add.f32 %ssa_159, %ssa_158, %ssa_66;	// vec1 32 ssa_159 = fadd ssa_158, ssa_66

	.reg .f32 %ssa_160;
	rcp.approx.f32 %ssa_160, %ssa_159;	// vec1 32 ssa_160 = frcp ssa_159

	.reg .f32 %ssa_161;
	mul.f32 %ssa_161, %ssa_156, %ssa_160;	// vec1 32 ssa_161 = fmul ssa_156, ssa_160

	.reg .f32 %ssa_162;
	mul.f32 %ssa_162, %ssa_64, %ssa_161;	// vec1 32 ssa_162 = fmul ssa_64, ssa_161

	.reg .f32 %ssa_163;
	add.f32 %ssa_163, %ssa_162, %ssa_64;	// vec1 32 ssa_163 = fadd ssa_162, ssa_64

	.reg .pred %ssa_164;
	setp.lt.f32 %ssa_164, %ssa_130, %ssa_129;	// vec1  1 ssa_164 = flt ssa_130, ssa_129

	.reg  .f32 %ssa_165;
	selp.f32 %ssa_165, %ssa_163, %ssa_147, %ssa_164; // vec1 32 ssa_165 = bcsel ssa_164, ssa_163, ssa_147

	.reg .f32 %ssa_166;
	add.f32 %ssa_166, %ssa_128, %ssa_68;	// vec1 32 ssa_166 = fadd ssa_128, ssa_68

	.reg .f32 %ssa_167;
	mov.f32 %ssa_167, 0F3e22f983; // vec1 32 ssa_167 = load_const (0x3e22f983 /* 0.159155 */)
	.reg .b32 %ssa_167_bits;
	mov.b32 %ssa_167_bits, 0F3e22f983;

	.reg .f32 %ssa_168;
	mul.f32 %ssa_168, %ssa_166, %ssa_167;	// vec1 32 ssa_168 = fmul ssa_166, ssa_167

	.reg .f32 %ssa_169;
	add.f32 %ssa_169, %ssa_165, %ssa_67;	// vec1 32 ssa_169 = fadd ssa_165, ssa_67

	.reg .f32 %ssa_170;
	mov.f32 %ssa_170, 0F3ea2f983; // vec1 32 ssa_170 = load_const (0x3ea2f983 /* 0.318310 */)
	.reg .b32 %ssa_170_bits;
	mov.b32 %ssa_170_bits, 0F3ea2f983;

	.reg .f32 %ssa_171;
	mul.f32 %ssa_171, %ssa_169, %ssa_170;	// vec1 32 ssa_171 = fmul ssa_169, ssa_170

	.reg .f32 %ssa_172;
	neg.f32 %ssa_172, %ssa_171;	// vec1 32 ssa_172 = fneg ssa_171

	.reg .f32 %ssa_173;
	add.f32 %ssa_173, %ssa_66, %ssa_172;	// vec1 32 ssa_173 = fadd ssa_66, ssa_172

	.reg .f32 %ssa_174_0;
	.reg .f32 %ssa_174_1;
	mov.f32 %ssa_174_0, %ssa_168;
	mov.f32 %ssa_174_1, %ssa_173; // vec2 32 ssa_174 = vec2 ssa_168, ssa_173

	.reg .b64 %ssa_175;
	mov.b64 %ssa_175, %Ray; // vec1 32 ssa_175 = deref_var &Ray (shader_call_data RayPayload) 

	.reg .b64 %ssa_176;
	add.u64 %ssa_176, %ssa_175, 32; // vec1 32 ssa_176 = deref_struct &ssa_175->RandomSeed (shader_call_data uint) /* &Ray.RandomSeed */

	.reg  .u32 %ssa_177;
	ld.global.u32 %ssa_177, [%ssa_176]; // vec1 32 ssa_177 = intrinsic load_deref (%ssa_176) (0) /* access=0 */

	.reg .f32 %ssa_178;
	mul.f32 %ssa_178, %ssa_49_0, %ssa_49_0; // vec1 32 ssa_178 = fmul ssa_49.x, ssa_49.x

	.reg .f32 %ssa_179;
	mul.f32 %ssa_179, %ssa_49_1, %ssa_49_1; // vec1 32 ssa_179 = fmul ssa_49.y, ssa_49.y

	.reg .f32 %ssa_180;
	mul.f32 %ssa_180, %ssa_49_2, %ssa_49_2; // vec1 32 ssa_180 = fmul ssa_49.z, ssa_49.z

	.reg .f32 %ssa_181_0;
	.reg .f32 %ssa_181_1;
	.reg .f32 %ssa_181_2;
	.reg .f32 %ssa_181_3;
	mov.f32 %ssa_181_0, %ssa_178;
	mov.f32 %ssa_181_1, %ssa_179;
	mov.f32 %ssa_181_2, %ssa_180; // vec3 32 ssa_181 = vec3 ssa_178, ssa_179, ssa_180

	.reg .f32 %ssa_182;
	add.f32 %ssa_182, %ssa_181_0, %ssa_181_1;
	add.f32 %ssa_182, %ssa_182, %ssa_181_2; // vec1 32 ssa_182 = fsum3 ssa_181

	.reg .f32 %ssa_183;
	rsqrt.approx.f32 %ssa_183, %ssa_182;	// vec1 32 ssa_183 = frsq ssa_182

	.reg .f32 %ssa_184;
	mul.f32 %ssa_184, %ssa_49_0, %ssa_183; // vec1 32 ssa_184 = fmul ssa_49.x, ssa_183

	.reg .f32 %ssa_185;
	mul.f32 %ssa_185, %ssa_49_1, %ssa_183; // vec1 32 ssa_185 = fmul ssa_49.y, ssa_183

	.reg .f32 %ssa_186;
	mul.f32 %ssa_186, %ssa_49_2, %ssa_183; // vec1 32 ssa_186 = fmul ssa_49.z, ssa_183

	.reg .pred %ssa_187;
	setp.eq.s32 %ssa_187, %ssa_40, %ssa_1_bits; // vec1  1 ssa_187 = ieq ssa_40, ssa_1

	// succs: block_1 block_10 
	// end_block block_0:
	//if
	@!%ssa_187 bra else_16;
	
		// start_block block_1:
		// preds: block_0 
		.reg .f32 %ssa_188_0;
		.reg .f32 %ssa_188_1;
		.reg .f32 %ssa_188_2;
		.reg .f32 %ssa_188_3;
	mov.f32 %ssa_188_0, 0F3f800000;
	mov.f32 %ssa_188_1, 0F3f800000;
	mov.f32 %ssa_188_2, 0F3f800000;
	mov.f32 %ssa_188_3, 0F3f800000;
		// vec4 32 ssa_188 = load_const (0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */)

		.reg .f32 %ssa_189;
		mul.f32 %ssa_189, %ssa_184, %ssa_63;	// vec1 32 ssa_189 = fmul ssa_184, ssa_63

		.reg .f32 %ssa_190;
		mul.f32 %ssa_190, %ssa_185, %ssa_64;	// vec1 32 ssa_190 = fmul ssa_185, ssa_64

		.reg .f32 %ssa_191;
		mul.f32 %ssa_191, %ssa_186, %ssa_65;	// vec1 32 ssa_191 = fmul ssa_186, ssa_65

		.reg .f32 %ssa_192_0;
		.reg .f32 %ssa_192_1;
		.reg .f32 %ssa_192_2;
		.reg .f32 %ssa_192_3;
		mov.f32 %ssa_192_0, %ssa_189;
		mov.f32 %ssa_192_1, %ssa_190;
		mov.f32 %ssa_192_2, %ssa_191; // vec3 32 ssa_192 = vec3 ssa_189, ssa_190, ssa_191

		.reg .f32 %ssa_193;
		add.f32 %ssa_193, %ssa_192_0, %ssa_192_1;
		add.f32 %ssa_193, %ssa_193, %ssa_192_2; // vec1 32 ssa_193 = fsum3 ssa_192

		.reg .pred %ssa_194;
		setp.lt.f32 %ssa_194, %ssa_193, %ssa_1;	// vec1  1 ssa_194 = flt! ssa_193, ssa_1

		.reg .pred %ssa_195;
		setp.ge.s32 %ssa_195, %ssa_34, %ssa_1_bits; // vec1  1 ssa_195 = ige ssa_34, ssa_1

		// succs: block_2 block_3 
		// end_block block_1:
		//if
		@!%ssa_195 bra else_17;
		
			// start_block block_2:
			// preds: block_1 
			.reg .b64 %ssa_196;
	mov.b64 %ssa_196, %TextureSamplers; // vec1 32 ssa_196 = deref_var &TextureSamplers (uniform sampler2D[]) 

			.reg .b64 %ssa_197;
			.reg .u32 %ssa_197_array_index_32;
			.reg .u64 %ssa_197_array_index_64;
			cvt.u32.s32 %ssa_197_array_index_32, %ssa_34;
			mul.wide.u32 %ssa_197_array_index_64, %ssa_197_array_index_32, 32;
			add.u64 %ssa_197, %ssa_196, %ssa_197_array_index_64; // vec1 32 ssa_197 = deref_array &(*ssa_196)[ssa_34] (uniform sampler2D) /* &TextureSamplers[ssa_34] */

			.reg .f32 %ssa_198_0;
			.reg .f32 %ssa_198_1;
			.reg .f32 %ssa_198_2;
			.reg .f32 %ssa_198_3;
	txl %ssa_197, %ssa_197, %ssa_198_0, %ssa_198_1, %ssa_198_2, %ssa_198_3, %ssa_174_0, %ssa_174_1, %ssa_1; // vec4 32 ssa_198 = (float32)txl ssa_197 (texture_deref), ssa_197 (sampler_deref), ssa_174 (coord), ssa_1 (lod), texture non-uniform, sampler non-uniform

			.reg .f32 %ssa_427;
			mov.f32 %ssa_427, %ssa_198_0; // vec1 32 ssa_427 = mov ssa_198.x

			.reg .f32 %ssa_430;
			mov.f32 %ssa_430, %ssa_198_1; // vec1 32 ssa_430 = mov ssa_198.y

			.reg .f32 %ssa_433;
			mov.f32 %ssa_433, %ssa_198_2; // vec1 32 ssa_433 = mov ssa_198.z

			.reg .f32 %ssa_436;
			mov.f32 %ssa_436, %ssa_198_3; // vec1 32 ssa_436 = mov ssa_198.w

			mov.f32 %ssa_429, %ssa_427; // vec1 32 ssa_429 = phi block_2: ssa_427, block_3: ssa_544
			mov.f32 %ssa_432, %ssa_430; // vec1 32 ssa_432 = phi block_2: ssa_430, block_3: ssa_545
			mov.f32 %ssa_435, %ssa_433; // vec1 32 ssa_435 = phi block_2: ssa_433, block_3: ssa_546
			mov.f32 %ssa_438, %ssa_436; // vec1 32 ssa_438 = phi block_2: ssa_436, block_3: ssa_547
			// succs: block_4 
			// end_block block_2:
			bra end_if_17;
		
		else_17: 
			// start_block block_3:
			// preds: block_1 
			.reg .f32 %ssa_544;
	mov.f32 %ssa_544, 0F3f800000; // vec1 32 ssa_544 = load_const (0x3f800000 /* 1.000000 */)
			.reg .b32 %ssa_544_bits;
	mov.b32 %ssa_544_bits, 0F3f800000;

			.reg .f32 %ssa_545;
	mov.f32 %ssa_545, 0F3f800000; // vec1 32 ssa_545 = load_const (0x3f800000 /* 1.000000 */)
			.reg .b32 %ssa_545_bits;
	mov.b32 %ssa_545_bits, 0F3f800000;

			.reg .f32 %ssa_546;
	mov.f32 %ssa_546, 0F3f800000; // vec1 32 ssa_546 = load_const (0x3f800000 /* 1.000000 */)
			.reg .b32 %ssa_546_bits;
	mov.b32 %ssa_546_bits, 0F3f800000;

			.reg .f32 %ssa_547;
	mov.f32 %ssa_547, 0F3f800000; // vec1 32 ssa_547 = load_const (0x3f800000 /* 1.000000 */)
			.reg .b32 %ssa_547_bits;
	mov.b32 %ssa_547_bits, 0F3f800000;

			mov.f32 %ssa_429, %ssa_544; // vec1 32 ssa_429 = phi block_2: ssa_427, block_3: ssa_544
			mov.f32 %ssa_432, %ssa_545; // vec1 32 ssa_432 = phi block_2: ssa_430, block_3: ssa_545
			mov.f32 %ssa_435, %ssa_546; // vec1 32 ssa_435 = phi block_2: ssa_433, block_3: ssa_546
			mov.f32 %ssa_438, %ssa_547; // vec1 32 ssa_438 = phi block_2: ssa_436, block_3: ssa_547
			// succs: block_4 
			// end_block block_3:
		end_if_17:
		// start_block block_4:
		// preds: block_2 block_3 




		.reg .b32 %ssa_439_0;
		.reg .b32 %ssa_439_1;
		.reg .b32 %ssa_439_2;
		.reg .b32 %ssa_439_3;
		mov.b32 %ssa_439_0, %ssa_429;
		mov.b32 %ssa_439_1, %ssa_432;
		mov.b32 %ssa_439_2, %ssa_435;
		mov.b32 %ssa_439_3, %ssa_438; // vec4 32 ssa_439 = vec4 ssa_429, ssa_432, ssa_435, ssa_438

		.reg .f32 %ssa_200;
		mul.f32 %ssa_200, %ssa_32_0, %ssa_439_0; // vec1 32 ssa_200 = fmul ssa_32.x, ssa_439.x

		.reg .f32 %ssa_201;
		mul.f32 %ssa_201, %ssa_32_1, %ssa_439_1; // vec1 32 ssa_201 = fmul ssa_32.y, ssa_439.y

		.reg .f32 %ssa_202;
		mul.f32 %ssa_202, %ssa_32_2, %ssa_439_2; // vec1 32 ssa_202 = fmul ssa_32.z, ssa_439.z

	mov.s32 %ssa_203, %ssa_177; // vec1 32 ssa_203 = phi block_4: ssa_177, block_8: ssa_216
		// succs: block_5 
		// end_block block_4:
		loop_5: 
			// start_block block_5:
			// preds: block_4 block_8 

			.reg .f32 %ssa_204;
	mov.f32 %ssa_204, 0F00ffffff; // vec1 32 ssa_204 = load_const (0x00ffffff /* 0.000000 */)
			.reg .b32 %ssa_204_bits;
	mov.b32 %ssa_204_bits, 0F00ffffff;

			.reg .f32 %ssa_205;
	mov.f32 %ssa_205, 0F3c6ef35f; // vec1 32 ssa_205 = load_const (0x3c6ef35f /* 0.014584 */)
			.reg .b32 %ssa_205_bits;
	mov.b32 %ssa_205_bits, 0F3c6ef35f;

			.reg .f32 %ssa_206;
	mov.f32 %ssa_206, 0F0019660d; // vec1 32 ssa_206 = load_const (0x0019660d /* 0.000000 */)
			.reg .b32 %ssa_206_bits;
	mov.b32 %ssa_206_bits, 0F0019660d;

			.reg .s32 %ssa_207;
			mul.lo.s32 %ssa_207, %ssa_206_bits, %ssa_203; // vec1 32 ssa_207 = imul ssa_206, ssa_203

			.reg .s32 %ssa_208;
			add.s32 %ssa_208, %ssa_207, %ssa_205_bits; // vec1 32 ssa_208 = iadd ssa_207, ssa_205

			.reg .u32 %ssa_209;
			and.b32 %ssa_209, %ssa_208, %ssa_204;	// vec1 32 ssa_209 = iand ssa_208, ssa_204

			.reg .f32 %ssa_210;
			cvt.rn.f32.u32 %ssa_210, %ssa_209;	// vec1 32 ssa_210 = u2f32 ssa_209

			.reg .s32 %ssa_211;
			mul.lo.s32 %ssa_211, %ssa_206_bits, %ssa_208; // vec1 32 ssa_211 = imul ssa_206, ssa_208

			.reg .s32 %ssa_212;
			add.s32 %ssa_212, %ssa_211, %ssa_205_bits; // vec1 32 ssa_212 = iadd ssa_211, ssa_205

			.reg .u32 %ssa_213;
			and.b32 %ssa_213, %ssa_212, %ssa_204;	// vec1 32 ssa_213 = iand ssa_212, ssa_204

			.reg .f32 %ssa_214;
			cvt.rn.f32.u32 %ssa_214, %ssa_213;	// vec1 32 ssa_214 = u2f32 ssa_213

			.reg .s32 %ssa_215;
			mul.lo.s32 %ssa_215, %ssa_206_bits, %ssa_212; // vec1 32 ssa_215 = imul ssa_206, ssa_212

			.reg .s32 %ssa_216;
			add.s32 %ssa_216, %ssa_215, %ssa_205_bits; // vec1 32 ssa_216 = iadd ssa_215, ssa_205

			.reg .u32 %ssa_217;
			and.b32 %ssa_217, %ssa_216, %ssa_204;	// vec1 32 ssa_217 = iand ssa_216, ssa_204

			.reg .f32 %ssa_218;
			cvt.rn.f32.u32 %ssa_218, %ssa_217;	// vec1 32 ssa_218 = u2f32 ssa_217

			.reg .f32 %ssa_219;
	mov.f32 %ssa_219, 0F34000000; // vec1 32 ssa_219 = load_const (0x34000000 /* 0.000000 */)
			.reg .b32 %ssa_219_bits;
	mov.b32 %ssa_219_bits, 0F34000000;

			.reg .f32 %ssa_220;
			mul.f32 %ssa_220, %ssa_219, %ssa_210;	// vec1 32 ssa_220 = fmul ssa_219, ssa_210

			.reg .f32 %ssa_221;
			mul.f32 %ssa_221, %ssa_219, %ssa_214;	// vec1 32 ssa_221 = fmul ssa_219, ssa_214

			.reg .f32 %ssa_222;
			mul.f32 %ssa_222, %ssa_219, %ssa_218;	// vec1 32 ssa_222 = fmul ssa_219, ssa_218

			.reg .f32 %ssa_223_0;
			.reg .f32 %ssa_223_1;
			.reg .f32 %ssa_223_2;
			.reg .f32 %ssa_223_3;
	mov.f32 %ssa_223_0, 0Fbf800000;
	mov.f32 %ssa_223_1, 0Fbf800000;
	mov.f32 %ssa_223_2, 0Fbf800000;
	mov.f32 %ssa_223_3, 0F00000000;
		// vec3 32 ssa_223 = load_const (0xbf800000 /* -1.000000 */, 0xbf800000 /* -1.000000 */, 0xbf800000 /* -1.000000 */)

			.reg .f32 %ssa_224;
			add.f32 %ssa_224, %ssa_220, %ssa_223_0; // vec1 32 ssa_224 = fadd ssa_220, ssa_223.x

			.reg .f32 %ssa_225;
			add.f32 %ssa_225, %ssa_221, %ssa_223_1; // vec1 32 ssa_225 = fadd ssa_221, ssa_223.y

			.reg .f32 %ssa_226;
			add.f32 %ssa_226, %ssa_222, %ssa_223_2; // vec1 32 ssa_226 = fadd ssa_222, ssa_223.z

			.reg .f32 %ssa_227;
			mul.f32 %ssa_227, %ssa_224, %ssa_224;	// vec1 32 ssa_227 = fmul ssa_224, ssa_224

			.reg .f32 %ssa_228;
			mul.f32 %ssa_228, %ssa_225, %ssa_225;	// vec1 32 ssa_228 = fmul ssa_225, ssa_225

			.reg .f32 %ssa_229;
			mul.f32 %ssa_229, %ssa_226, %ssa_226;	// vec1 32 ssa_229 = fmul ssa_226, ssa_226

			.reg .f32 %ssa_230_0;
			.reg .f32 %ssa_230_1;
			.reg .f32 %ssa_230_2;
			.reg .f32 %ssa_230_3;
			mov.f32 %ssa_230_0, %ssa_227;
			mov.f32 %ssa_230_1, %ssa_228;
			mov.f32 %ssa_230_2, %ssa_229; // vec3 32 ssa_230 = vec3 ssa_227, ssa_228, ssa_229

			.reg .f32 %ssa_231;
			add.f32 %ssa_231, %ssa_230_0, %ssa_230_1;
			add.f32 %ssa_231, %ssa_231, %ssa_230_2; // vec1 32 ssa_231 = fsum3 ssa_230

			.reg .pred %ssa_232;
			setp.lt.f32 %ssa_232, %ssa_231, %ssa_66;	// vec1  1 ssa_232 = flt! ssa_231, ssa_66

			// succs: block_6 block_7 
			// end_block block_5:
			//if
			@!%ssa_232 bra else_18;
			
				// start_block block_6:
				// preds: block_5 
				bra loop_5_exit;

				// succs: block_9 
				// end_block block_6:
				bra end_if_18;
			
			else_18: 
				// start_block block_7:
				// preds: block_5 
				// succs: block_8 
				// end_block block_7:
			end_if_18:
			// start_block block_8:
			// preds: block_7 
			mov.s32 %ssa_203, %ssa_216; // vec1 32 ssa_203 = phi block_4: ssa_177, block_8: ssa_216
			// succs: block_5 
			// end_block block_8:
			bra loop_5;
		
		loop_5_exit:
		// start_block block_9:
		// preds: block_6 
		.reg .f32 %ssa_233;
		add.f32 %ssa_233, %ssa_63, %ssa_224;	// vec1 32 ssa_233 = fadd ssa_63, ssa_224

		.reg .f32 %ssa_234;
		add.f32 %ssa_234, %ssa_64, %ssa_225;	// vec1 32 ssa_234 = fadd ssa_64, ssa_225

		.reg .f32 %ssa_235;
		add.f32 %ssa_235, %ssa_65, %ssa_226;	// vec1 32 ssa_235 = fadd ssa_65, ssa_226

		.reg .f32 %ssa_236;
		selp.f32 %ssa_236, 0F3f800000, 0F00000000, %ssa_194; // vec1 32 ssa_236 = b2f32 ssa_194

		.reg .f32 %ssa_237_0;
		.reg .f32 %ssa_237_1;
		.reg .f32 %ssa_237_2;
		.reg .f32 %ssa_237_3;
		mov.f32 %ssa_237_0, %ssa_200;
		mov.f32 %ssa_237_1, %ssa_201;
		mov.f32 %ssa_237_2, %ssa_202;
		mov.f32 %ssa_237_3, %ssa_48; // vec4 32 ssa_237 = vec4 ssa_200, ssa_201, ssa_202, ssa_48

		.reg .f32 %ssa_238_0;
		.reg .f32 %ssa_238_1;
		.reg .f32 %ssa_238_2;
		.reg .f32 %ssa_238_3;
		mov.f32 %ssa_238_0, %ssa_233;
		mov.f32 %ssa_238_1, %ssa_234;
		mov.f32 %ssa_238_2, %ssa_235;
		mov.f32 %ssa_238_3, %ssa_236; // vec4 32 ssa_238 = vec4 ssa_233, ssa_234, ssa_235, ssa_236

		.reg .f32 %ssa_518;
		mov.f32 %ssa_518, %ssa_238_0; // vec1 32 ssa_518 = mov ssa_238.x

		.reg .f32 %ssa_521;
		mov.f32 %ssa_521, %ssa_238_1; // vec1 32 ssa_521 = mov ssa_238.y

		.reg .f32 %ssa_524;
		mov.f32 %ssa_524, %ssa_238_2; // vec1 32 ssa_524 = mov ssa_238.z

		.reg .f32 %ssa_527;
		mov.f32 %ssa_527, %ssa_238_3; // vec1 32 ssa_527 = mov ssa_238.w

		.reg .f32 %ssa_531;
		mov.f32 %ssa_531, %ssa_237_0; // vec1 32 ssa_531 = mov ssa_237.x

		.reg .f32 %ssa_534;
		mov.f32 %ssa_534, %ssa_237_1; // vec1 32 ssa_534 = mov ssa_237.y

		.reg .f32 %ssa_537;
		mov.f32 %ssa_537, %ssa_237_2; // vec1 32 ssa_537 = mov ssa_237.z

		.reg .f32 %ssa_540;
		mov.f32 %ssa_540, %ssa_237_3; // vec1 32 ssa_540 = mov ssa_237.w

			mov.s32 %ssa_422, %ssa_216; // vec1 32 ssa_422 = phi block_9: ssa_216, block_30: ssa_419
		mov.b32 %ssa_520, %ssa_518; // vec1 32 ssa_520 = phi block_9: ssa_518, block_30: ssa_519
		mov.b32 %ssa_523, %ssa_521; // vec1 32 ssa_523 = phi block_9: ssa_521, block_30: ssa_522
		mov.b32 %ssa_526, %ssa_524; // vec1 32 ssa_526 = phi block_9: ssa_524, block_30: ssa_525
		mov.b32 %ssa_529, %ssa_527; // vec1 32 ssa_529 = phi block_9: ssa_527, block_30: ssa_528
		mov.b32 %ssa_533, %ssa_531; // vec1 32 ssa_533 = phi block_9: ssa_531, block_30: ssa_532
		mov.b32 %ssa_536, %ssa_534; // vec1 32 ssa_536 = phi block_9: ssa_534, block_30: ssa_535
		mov.b32 %ssa_539, %ssa_537; // vec1 32 ssa_539 = phi block_9: ssa_537, block_30: ssa_538
		mov.b32 %ssa_542, %ssa_540; // vec1 32 ssa_542 = phi block_9: ssa_540, block_30: ssa_541
		// succs: block_31 
		// end_block block_9:
		bra end_if_16;
	
	else_16: 
		// start_block block_10:
		// preds: block_0 
		.reg .f32 %ssa_239;
	mov.f32 %ssa_239, 0F00000001; // vec1 32 ssa_239 = load_const (0x00000001 /* 0.000000 */)
		.reg .b32 %ssa_239_bits;
	mov.b32 %ssa_239_bits, 0F00000001;

		.reg .pred %ssa_240;
		setp.eq.s32 %ssa_240, %ssa_40, %ssa_239_bits; // vec1  1 ssa_240 = ieq ssa_40, ssa_239

		// succs: block_11 block_20 
		// end_block block_10:
		//if
		@!%ssa_240 bra else_19;
		
			// start_block block_11:
			// preds: block_10 
			.reg .f32 %ssa_241_0;
			.reg .f32 %ssa_241_1;
			.reg .f32 %ssa_241_2;
			.reg .f32 %ssa_241_3;
	mov.f32 %ssa_241_0, 0F3f800000;
	mov.f32 %ssa_241_1, 0F3f800000;
	mov.f32 %ssa_241_2, 0F3f800000;
	mov.f32 %ssa_241_3, 0F3f800000;
		// vec4 32 ssa_241 = load_const (0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */)

			.reg .f32 %ssa_242;
	mov.f32 %ssa_242, 0F40000000; // vec1 32 ssa_242 = load_const (0x40000000 /* 2.000000 */)
			.reg .b32 %ssa_242_bits;
	mov.b32 %ssa_242_bits, 0F40000000;

			.reg .f32 %ssa_243;
			mul.f32 %ssa_243, %ssa_184, %ssa_63;	// vec1 32 ssa_243 = fmul ssa_184, ssa_63

			.reg .f32 %ssa_244;
			mul.f32 %ssa_244, %ssa_185, %ssa_64;	// vec1 32 ssa_244 = fmul ssa_185, ssa_64

			.reg .f32 %ssa_245;
			mul.f32 %ssa_245, %ssa_186, %ssa_65;	// vec1 32 ssa_245 = fmul ssa_186, ssa_65

			.reg .f32 %ssa_246_0;
			.reg .f32 %ssa_246_1;
			.reg .f32 %ssa_246_2;
			.reg .f32 %ssa_246_3;
			mov.f32 %ssa_246_0, %ssa_243;
			mov.f32 %ssa_246_1, %ssa_244;
			mov.f32 %ssa_246_2, %ssa_245; // vec3 32 ssa_246 = vec3 ssa_243, ssa_244, ssa_245

			.reg .f32 %ssa_247;
			add.f32 %ssa_247, %ssa_246_0, %ssa_246_1;
			add.f32 %ssa_247, %ssa_247, %ssa_246_2; // vec1 32 ssa_247 = fsum3 ssa_246

			.reg .f32 %ssa_248;
			mul.f32 %ssa_248, %ssa_247, %ssa_242;	// vec1 32 ssa_248 = fmul ssa_247, ssa_242

			.reg .f32 %ssa_249;
			mul.f32 %ssa_249, %ssa_63, %ssa_248;	// vec1 32 ssa_249 = fmul ssa_63, ssa_248

			.reg .f32 %ssa_250;
			neg.f32 %ssa_250, %ssa_249;	// vec1 32 ssa_250 = fneg ssa_249

			.reg .f32 %ssa_251;
			mul.f32 %ssa_251, %ssa_64, %ssa_248;	// vec1 32 ssa_251 = fmul ssa_64, ssa_248

			.reg .f32 %ssa_252;
			neg.f32 %ssa_252, %ssa_251;	// vec1 32 ssa_252 = fneg ssa_251

			.reg .f32 %ssa_253;
			mul.f32 %ssa_253, %ssa_65, %ssa_248;	// vec1 32 ssa_253 = fmul ssa_65, ssa_248

			.reg .f32 %ssa_254;
			neg.f32 %ssa_254, %ssa_253;	// vec1 32 ssa_254 = fneg ssa_253

			.reg .f32 %ssa_255;
			add.f32 %ssa_255, %ssa_250, %ssa_184;	// vec1 32 ssa_255 = fadd ssa_250, ssa_184

			.reg .f32 %ssa_256;
			add.f32 %ssa_256, %ssa_252, %ssa_185;	// vec1 32 ssa_256 = fadd ssa_252, ssa_185

			.reg .f32 %ssa_257;
			add.f32 %ssa_257, %ssa_254, %ssa_186;	// vec1 32 ssa_257 = fadd ssa_254, ssa_186

			.reg .f32 %ssa_258;
			mul.f32 %ssa_258, %ssa_255, %ssa_63;	// vec1 32 ssa_258 = fmul ssa_255, ssa_63

			.reg .f32 %ssa_259;
			mul.f32 %ssa_259, %ssa_256, %ssa_64;	// vec1 32 ssa_259 = fmul ssa_256, ssa_64

			.reg .f32 %ssa_260;
			mul.f32 %ssa_260, %ssa_257, %ssa_65;	// vec1 32 ssa_260 = fmul ssa_257, ssa_65

			.reg .f32 %ssa_261_0;
			.reg .f32 %ssa_261_1;
			.reg .f32 %ssa_261_2;
			.reg .f32 %ssa_261_3;
			mov.f32 %ssa_261_0, %ssa_258;
			mov.f32 %ssa_261_1, %ssa_259;
			mov.f32 %ssa_261_2, %ssa_260; // vec3 32 ssa_261 = vec3 ssa_258, ssa_259, ssa_260

			.reg .f32 %ssa_262;
			add.f32 %ssa_262, %ssa_261_0, %ssa_261_1;
			add.f32 %ssa_262, %ssa_262, %ssa_261_2; // vec1 32 ssa_262 = fsum3 ssa_261

			.reg .pred %ssa_263;
			setp.lt.f32 %ssa_263, %ssa_1, %ssa_262;	// vec1  1 ssa_263 = flt! ssa_1, ssa_262

			.reg .pred %ssa_264;
			setp.ge.s32 %ssa_264, %ssa_34, %ssa_1_bits; // vec1  1 ssa_264 = ige ssa_34, ssa_1

			// succs: block_12 block_13 
			// end_block block_11:
			//if
			@!%ssa_264 bra else_20;
			
				// start_block block_12:
				// preds: block_11 
				.reg .b64 %ssa_265;
	mov.b64 %ssa_265, %TextureSamplers; // vec1 32 ssa_265 = deref_var &TextureSamplers (uniform sampler2D[]) 

				.reg .b64 %ssa_266;
				.reg .u32 %ssa_266_array_index_32;
				.reg .u64 %ssa_266_array_index_64;
				cvt.u32.s32 %ssa_266_array_index_32, %ssa_34;
				mul.wide.u32 %ssa_266_array_index_64, %ssa_266_array_index_32, 32;
				add.u64 %ssa_266, %ssa_265, %ssa_266_array_index_64; // vec1 32 ssa_266 = deref_array &(*ssa_265)[ssa_34] (uniform sampler2D) /* &TextureSamplers[ssa_34] */

				.reg .f32 %ssa_267_0;
				.reg .f32 %ssa_267_1;
				.reg .f32 %ssa_267_2;
				.reg .f32 %ssa_267_3;
	txl %ssa_266, %ssa_266, %ssa_267_0, %ssa_267_1, %ssa_267_2, %ssa_267_3, %ssa_174_0, %ssa_174_1, %ssa_1; // vec4 32 ssa_267 = (float32)txl ssa_266 (texture_deref), ssa_266 (sampler_deref), ssa_174 (coord), ssa_1 (lod), texture non-uniform, sampler non-uniform

				.reg .f32 %ssa_440;
				mov.f32 %ssa_440, %ssa_267_0; // vec1 32 ssa_440 = mov ssa_267.x

				.reg .f32 %ssa_443;
				mov.f32 %ssa_443, %ssa_267_1; // vec1 32 ssa_443 = mov ssa_267.y

				.reg .f32 %ssa_446;
				mov.f32 %ssa_446, %ssa_267_2; // vec1 32 ssa_446 = mov ssa_267.z

				.reg .f32 %ssa_449;
				mov.f32 %ssa_449, %ssa_267_3; // vec1 32 ssa_449 = mov ssa_267.w

				mov.f32 %ssa_442, %ssa_440; // vec1 32 ssa_442 = phi block_12: ssa_440, block_13: ssa_548
				mov.f32 %ssa_445, %ssa_443; // vec1 32 ssa_445 = phi block_12: ssa_443, block_13: ssa_549
				mov.f32 %ssa_448, %ssa_446; // vec1 32 ssa_448 = phi block_12: ssa_446, block_13: ssa_550
				mov.f32 %ssa_451, %ssa_449; // vec1 32 ssa_451 = phi block_12: ssa_449, block_13: ssa_551
				// succs: block_14 
				// end_block block_12:
				bra end_if_20;
			
			else_20: 
				// start_block block_13:
				// preds: block_11 
				.reg .f32 %ssa_548;
	mov.f32 %ssa_548, 0F3f800000; // vec1 32 ssa_548 = load_const (0x3f800000 /* 1.000000 */)
				.reg .b32 %ssa_548_bits;
	mov.b32 %ssa_548_bits, 0F3f800000;

				.reg .f32 %ssa_549;
	mov.f32 %ssa_549, 0F3f800000; // vec1 32 ssa_549 = load_const (0x3f800000 /* 1.000000 */)
				.reg .b32 %ssa_549_bits;
	mov.b32 %ssa_549_bits, 0F3f800000;

				.reg .f32 %ssa_550;
	mov.f32 %ssa_550, 0F3f800000; // vec1 32 ssa_550 = load_const (0x3f800000 /* 1.000000 */)
				.reg .b32 %ssa_550_bits;
	mov.b32 %ssa_550_bits, 0F3f800000;

				.reg .f32 %ssa_551;
	mov.f32 %ssa_551, 0F3f800000; // vec1 32 ssa_551 = load_const (0x3f800000 /* 1.000000 */)
				.reg .b32 %ssa_551_bits;
	mov.b32 %ssa_551_bits, 0F3f800000;

				mov.f32 %ssa_442, %ssa_548; // vec1 32 ssa_442 = phi block_12: ssa_440, block_13: ssa_548
				mov.f32 %ssa_445, %ssa_549; // vec1 32 ssa_445 = phi block_12: ssa_443, block_13: ssa_549
				mov.f32 %ssa_448, %ssa_550; // vec1 32 ssa_448 = phi block_12: ssa_446, block_13: ssa_550
				mov.f32 %ssa_451, %ssa_551; // vec1 32 ssa_451 = phi block_12: ssa_449, block_13: ssa_551
				// succs: block_14 
				// end_block block_13:
			end_if_20:
			// start_block block_14:
			// preds: block_12 block_13 




			.reg .b32 %ssa_452_0;
			.reg .b32 %ssa_452_1;
			.reg .b32 %ssa_452_2;
			.reg .b32 %ssa_452_3;
			mov.b32 %ssa_452_0, %ssa_442;
			mov.b32 %ssa_452_1, %ssa_445;
			mov.b32 %ssa_452_2, %ssa_448;
			mov.b32 %ssa_452_3, %ssa_451; // vec4 32 ssa_452 = vec4 ssa_442, ssa_445, ssa_448, ssa_451

			.reg .f32 %ssa_269;
			mul.f32 %ssa_269, %ssa_32_0, %ssa_452_0; // vec1 32 ssa_269 = fmul ssa_32.x, ssa_452.x

			.reg .f32 %ssa_270;
			mul.f32 %ssa_270, %ssa_32_1, %ssa_452_1; // vec1 32 ssa_270 = fmul ssa_32.y, ssa_452.y

			.reg .f32 %ssa_271;
			mul.f32 %ssa_271, %ssa_32_2, %ssa_452_2; // vec1 32 ssa_271 = fmul ssa_32.z, ssa_452.z

	mov.s32 %ssa_272, %ssa_177; // vec1 32 ssa_272 = phi block_14: ssa_177, block_18: ssa_285
			// succs: block_15 
			// end_block block_14:
			loop_6: 
				// start_block block_15:
				// preds: block_14 block_18 

				.reg .f32 %ssa_273;
	mov.f32 %ssa_273, 0F00ffffff; // vec1 32 ssa_273 = load_const (0x00ffffff /* 0.000000 */)
				.reg .b32 %ssa_273_bits;
	mov.b32 %ssa_273_bits, 0F00ffffff;

				.reg .f32 %ssa_274;
	mov.f32 %ssa_274, 0F3c6ef35f; // vec1 32 ssa_274 = load_const (0x3c6ef35f /* 0.014584 */)
				.reg .b32 %ssa_274_bits;
	mov.b32 %ssa_274_bits, 0F3c6ef35f;

				.reg .f32 %ssa_275;
	mov.f32 %ssa_275, 0F0019660d; // vec1 32 ssa_275 = load_const (0x0019660d /* 0.000000 */)
				.reg .b32 %ssa_275_bits;
	mov.b32 %ssa_275_bits, 0F0019660d;

				.reg .s32 %ssa_276;
				mul.lo.s32 %ssa_276, %ssa_275_bits, %ssa_272; // vec1 32 ssa_276 = imul ssa_275, ssa_272

				.reg .s32 %ssa_277;
				add.s32 %ssa_277, %ssa_276, %ssa_274_bits; // vec1 32 ssa_277 = iadd ssa_276, ssa_274

				.reg .u32 %ssa_278;
				and.b32 %ssa_278, %ssa_277, %ssa_273;	// vec1 32 ssa_278 = iand ssa_277, ssa_273

				.reg .f32 %ssa_279;
				cvt.rn.f32.u32 %ssa_279, %ssa_278;	// vec1 32 ssa_279 = u2f32 ssa_278

				.reg .s32 %ssa_280;
				mul.lo.s32 %ssa_280, %ssa_275_bits, %ssa_277; // vec1 32 ssa_280 = imul ssa_275, ssa_277

				.reg .s32 %ssa_281;
				add.s32 %ssa_281, %ssa_280, %ssa_274_bits; // vec1 32 ssa_281 = iadd ssa_280, ssa_274

				.reg .u32 %ssa_282;
				and.b32 %ssa_282, %ssa_281, %ssa_273;	// vec1 32 ssa_282 = iand ssa_281, ssa_273

				.reg .f32 %ssa_283;
				cvt.rn.f32.u32 %ssa_283, %ssa_282;	// vec1 32 ssa_283 = u2f32 ssa_282

				.reg .s32 %ssa_284;
				mul.lo.s32 %ssa_284, %ssa_275_bits, %ssa_281; // vec1 32 ssa_284 = imul ssa_275, ssa_281

				.reg .s32 %ssa_285;
				add.s32 %ssa_285, %ssa_284, %ssa_274_bits; // vec1 32 ssa_285 = iadd ssa_284, ssa_274

				.reg .u32 %ssa_286;
				and.b32 %ssa_286, %ssa_285, %ssa_273;	// vec1 32 ssa_286 = iand ssa_285, ssa_273

				.reg .f32 %ssa_287;
				cvt.rn.f32.u32 %ssa_287, %ssa_286;	// vec1 32 ssa_287 = u2f32 ssa_286

				.reg .f32 %ssa_288;
	mov.f32 %ssa_288, 0F34000000; // vec1 32 ssa_288 = load_const (0x34000000 /* 0.000000 */)
				.reg .b32 %ssa_288_bits;
	mov.b32 %ssa_288_bits, 0F34000000;

				.reg .f32 %ssa_289;
				mul.f32 %ssa_289, %ssa_288, %ssa_279;	// vec1 32 ssa_289 = fmul ssa_288, ssa_279

				.reg .f32 %ssa_290;
				mul.f32 %ssa_290, %ssa_288, %ssa_283;	// vec1 32 ssa_290 = fmul ssa_288, ssa_283

				.reg .f32 %ssa_291;
				mul.f32 %ssa_291, %ssa_288, %ssa_287;	// vec1 32 ssa_291 = fmul ssa_288, ssa_287

				.reg .f32 %ssa_292_0;
				.reg .f32 %ssa_292_1;
				.reg .f32 %ssa_292_2;
				.reg .f32 %ssa_292_3;
	mov.f32 %ssa_292_0, 0Fbf800000;
	mov.f32 %ssa_292_1, 0Fbf800000;
	mov.f32 %ssa_292_2, 0Fbf800000;
	mov.f32 %ssa_292_3, 0F00000000;
		// vec3 32 ssa_292 = load_const (0xbf800000 /* -1.000000 */, 0xbf800000 /* -1.000000 */, 0xbf800000 /* -1.000000 */)

				.reg .f32 %ssa_293;
				add.f32 %ssa_293, %ssa_289, %ssa_292_0; // vec1 32 ssa_293 = fadd ssa_289, ssa_292.x

				.reg .f32 %ssa_294;
				add.f32 %ssa_294, %ssa_290, %ssa_292_1; // vec1 32 ssa_294 = fadd ssa_290, ssa_292.y

				.reg .f32 %ssa_295;
				add.f32 %ssa_295, %ssa_291, %ssa_292_2; // vec1 32 ssa_295 = fadd ssa_291, ssa_292.z

				.reg .f32 %ssa_296;
				mul.f32 %ssa_296, %ssa_293, %ssa_293;	// vec1 32 ssa_296 = fmul ssa_293, ssa_293

				.reg .f32 %ssa_297;
				mul.f32 %ssa_297, %ssa_294, %ssa_294;	// vec1 32 ssa_297 = fmul ssa_294, ssa_294

				.reg .f32 %ssa_298;
				mul.f32 %ssa_298, %ssa_295, %ssa_295;	// vec1 32 ssa_298 = fmul ssa_295, ssa_295

				.reg .f32 %ssa_299_0;
				.reg .f32 %ssa_299_1;
				.reg .f32 %ssa_299_2;
				.reg .f32 %ssa_299_3;
				mov.f32 %ssa_299_0, %ssa_296;
				mov.f32 %ssa_299_1, %ssa_297;
				mov.f32 %ssa_299_2, %ssa_298; // vec3 32 ssa_299 = vec3 ssa_296, ssa_297, ssa_298

				.reg .f32 %ssa_300;
				add.f32 %ssa_300, %ssa_299_0, %ssa_299_1;
				add.f32 %ssa_300, %ssa_300, %ssa_299_2; // vec1 32 ssa_300 = fsum3 ssa_299

				.reg .pred %ssa_301;
				setp.lt.f32 %ssa_301, %ssa_300, %ssa_66;	// vec1  1 ssa_301 = flt! ssa_300, ssa_66

				// succs: block_16 block_17 
				// end_block block_15:
				//if
				@!%ssa_301 bra else_21;
				
					// start_block block_16:
					// preds: block_15 
					bra loop_6_exit;

					// succs: block_19 
					// end_block block_16:
					bra end_if_21;
				
				else_21: 
					// start_block block_17:
					// preds: block_15 
					// succs: block_18 
					// end_block block_17:
				end_if_21:
				// start_block block_18:
				// preds: block_17 
				mov.s32 %ssa_272, %ssa_285; // vec1 32 ssa_272 = phi block_14: ssa_177, block_18: ssa_285
				// succs: block_15 
				// end_block block_18:
				bra loop_6;
			
			loop_6_exit:
			// start_block block_19:
			// preds: block_16 
			.reg .f32 %ssa_302;
			mul.f32 %ssa_302, %ssa_293, %ssa_36;	// vec1 32 ssa_302 = fmul ssa_293, ssa_36

			.reg .f32 %ssa_303;
			mul.f32 %ssa_303, %ssa_294, %ssa_36;	// vec1 32 ssa_303 = fmul ssa_294, ssa_36

			.reg .f32 %ssa_304;
			mul.f32 %ssa_304, %ssa_295, %ssa_36;	// vec1 32 ssa_304 = fmul ssa_295, ssa_36

			.reg .f32 %ssa_305;
			add.f32 %ssa_305, %ssa_255, %ssa_302;	// vec1 32 ssa_305 = fadd ssa_255, ssa_302

			.reg .f32 %ssa_306;
			add.f32 %ssa_306, %ssa_256, %ssa_303;	// vec1 32 ssa_306 = fadd ssa_256, ssa_303

			.reg .f32 %ssa_307;
			add.f32 %ssa_307, %ssa_257, %ssa_304;	// vec1 32 ssa_307 = fadd ssa_257, ssa_304

			.reg .f32 %ssa_308;
			selp.f32 %ssa_308, 0F3f800000, 0F00000000, %ssa_263; // vec1 32 ssa_308 = b2f32 ssa_263

			.reg .f32 %ssa_309_0;
			.reg .f32 %ssa_309_1;
			.reg .f32 %ssa_309_2;
			.reg .f32 %ssa_309_3;
			mov.f32 %ssa_309_0, %ssa_269;
			mov.f32 %ssa_309_1, %ssa_270;
			mov.f32 %ssa_309_2, %ssa_271;
			mov.f32 %ssa_309_3, %ssa_48; // vec4 32 ssa_309 = vec4 ssa_269, ssa_270, ssa_271, ssa_48

			.reg .f32 %ssa_310_0;
			.reg .f32 %ssa_310_1;
			.reg .f32 %ssa_310_2;
			.reg .f32 %ssa_310_3;
			mov.f32 %ssa_310_0, %ssa_305;
			mov.f32 %ssa_310_1, %ssa_306;
			mov.f32 %ssa_310_2, %ssa_307;
			mov.f32 %ssa_310_3, %ssa_308; // vec4 32 ssa_310 = vec4 ssa_305, ssa_306, ssa_307, ssa_308

			.reg .f32 %ssa_492;
			mov.f32 %ssa_492, %ssa_310_0; // vec1 32 ssa_492 = mov ssa_310.x

			.reg .f32 %ssa_495;
			mov.f32 %ssa_495, %ssa_310_1; // vec1 32 ssa_495 = mov ssa_310.y

			.reg .f32 %ssa_498;
			mov.f32 %ssa_498, %ssa_310_2; // vec1 32 ssa_498 = mov ssa_310.z

			.reg .f32 %ssa_501;
			mov.f32 %ssa_501, %ssa_310_3; // vec1 32 ssa_501 = mov ssa_310.w

			.reg .f32 %ssa_505;
			mov.f32 %ssa_505, %ssa_309_0; // vec1 32 ssa_505 = mov ssa_309.x

			.reg .f32 %ssa_508;
			mov.f32 %ssa_508, %ssa_309_1; // vec1 32 ssa_508 = mov ssa_309.y

			.reg .f32 %ssa_511;
			mov.f32 %ssa_511, %ssa_309_2; // vec1 32 ssa_511 = mov ssa_309.z

			.reg .f32 %ssa_514;
			mov.f32 %ssa_514, %ssa_309_3; // vec1 32 ssa_514 = mov ssa_309.w

				mov.s32 %ssa_419, %ssa_285; // vec1 32 ssa_419 = phi block_19: ssa_285, block_29: ssa_416
			mov.b32 %ssa_494, %ssa_492; // vec1 32 ssa_494 = phi block_19: ssa_492, block_29: ssa_493
			mov.b32 %ssa_497, %ssa_495; // vec1 32 ssa_497 = phi block_19: ssa_495, block_29: ssa_496
			mov.b32 %ssa_500, %ssa_498; // vec1 32 ssa_500 = phi block_19: ssa_498, block_29: ssa_499
			mov.b32 %ssa_503, %ssa_501; // vec1 32 ssa_503 = phi block_19: ssa_501, block_29: ssa_502
			mov.b32 %ssa_507, %ssa_505; // vec1 32 ssa_507 = phi block_19: ssa_505, block_29: ssa_506
			mov.b32 %ssa_510, %ssa_508; // vec1 32 ssa_510 = phi block_19: ssa_508, block_29: ssa_509
			mov.b32 %ssa_513, %ssa_511; // vec1 32 ssa_513 = phi block_19: ssa_511, block_29: ssa_512
			mov.b32 %ssa_516, %ssa_514; // vec1 32 ssa_516 = phi block_19: ssa_514, block_29: ssa_515
			// succs: block_30 
			// end_block block_19:
			bra end_if_19;
		
		else_19: 
			// start_block block_20:
			// preds: block_10 
			.reg .f32 %ssa_311;
	mov.f32 %ssa_311, 0F00000002; // vec1 32 ssa_311 = load_const (0x00000002 /* 0.000000 */)
			.reg .b32 %ssa_311_bits;
	mov.b32 %ssa_311_bits, 0F00000002;

			.reg .pred %ssa_312;
			setp.eq.s32 %ssa_312, %ssa_40, %ssa_311_bits; // vec1  1 ssa_312 = ieq ssa_40, ssa_311

			// succs: block_21 block_28 
			// end_block block_20:
			//if
			@!%ssa_312 bra else_22;
			
				// start_block block_21:
				// preds: block_20 
				.reg .f32 %ssa_313_0;
				.reg .f32 %ssa_313_1;
				.reg .f32 %ssa_313_2;
				.reg .f32 %ssa_313_3;
	mov.f32 %ssa_313_0, 0F3f800000;
	mov.f32 %ssa_313_1, 0F3f800000;
	mov.f32 %ssa_313_2, 0F3f800000;
	mov.f32 %ssa_313_3, 0F3f800000;
		// vec4 32 ssa_313 = load_const (0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */, 0x3f800000 /* 1.000000 */)

				.reg .f32 %ssa_314;
				mul.f32 %ssa_314, %ssa_184, %ssa_63;	// vec1 32 ssa_314 = fmul ssa_184, ssa_63

				.reg .f32 %ssa_315;
				mul.f32 %ssa_315, %ssa_185, %ssa_64;	// vec1 32 ssa_315 = fmul ssa_185, ssa_64

				.reg .f32 %ssa_316;
				mul.f32 %ssa_316, %ssa_186, %ssa_65;	// vec1 32 ssa_316 = fmul ssa_186, ssa_65

				.reg .f32 %ssa_317_0;
				.reg .f32 %ssa_317_1;
				.reg .f32 %ssa_317_2;
				.reg .f32 %ssa_317_3;
				mov.f32 %ssa_317_0, %ssa_314;
				mov.f32 %ssa_317_1, %ssa_315;
				mov.f32 %ssa_317_2, %ssa_316; // vec3 32 ssa_317 = vec3 ssa_314, ssa_315, ssa_316

				.reg .f32 %ssa_318;
				add.f32 %ssa_318, %ssa_317_0, %ssa_317_1;
				add.f32 %ssa_318, %ssa_318, %ssa_317_2; // vec1 32 ssa_318 = fsum3 ssa_317

				.reg .pred %ssa_319;
				setp.lt.f32 %ssa_319, %ssa_1, %ssa_318;	// vec1  1 ssa_319 = flt! ssa_1, ssa_318

				.reg .f32 %ssa_320;
				neg.f32 %ssa_320, %ssa_63;	// vec1 32 ssa_320 = fneg ssa_63

				.reg .f32 %ssa_321;
				neg.f32 %ssa_321, %ssa_64;	// vec1 32 ssa_321 = fneg ssa_64

				.reg .f32 %ssa_322;
				neg.f32 %ssa_322, %ssa_65;	// vec1 32 ssa_322 = fneg ssa_65

				.reg  .f32 %ssa_323;
				selp.f32 %ssa_323, %ssa_320, %ssa_63, %ssa_319; // vec1 32 ssa_323 = bcsel ssa_319, ssa_320, ssa_63

				.reg  .f32 %ssa_324;
				selp.f32 %ssa_324, %ssa_321, %ssa_64, %ssa_319; // vec1 32 ssa_324 = bcsel ssa_319, ssa_321, ssa_64

				.reg  .f32 %ssa_325;
				selp.f32 %ssa_325, %ssa_322, %ssa_65, %ssa_319; // vec1 32 ssa_325 = bcsel ssa_319, ssa_322, ssa_65

				.reg .f32 %ssa_326;
				rcp.approx.f32 %ssa_326, %ssa_38;	// vec1 32 ssa_326 = frcp ssa_38

				.reg  .f32 %ssa_327;
				selp.f32 %ssa_327, %ssa_38, %ssa_326, %ssa_319; // vec1 32 ssa_327 = bcsel ssa_319, ssa_38, ssa_326

				.reg .f32 %ssa_328;
				mul.f32 %ssa_328, %ssa_38, %ssa_318;	// vec1 32 ssa_328 = fmul ssa_38, ssa_318

				.reg .f32 %ssa_329;
				neg.f32 %ssa_329, %ssa_318;	// vec1 32 ssa_329 = fneg ssa_318

				.reg  .f32 %ssa_330;
				selp.f32 %ssa_330, %ssa_328, %ssa_329, %ssa_319; // vec1 32 ssa_330 = bcsel ssa_319, ssa_328, ssa_329

				.reg .f32 %ssa_331;
				mul.f32 %ssa_331, %ssa_323, %ssa_184;	// vec1 32 ssa_331 = fmul ssa_323, ssa_184

				.reg .f32 %ssa_332;
				mul.f32 %ssa_332, %ssa_324, %ssa_185;	// vec1 32 ssa_332 = fmul ssa_324, ssa_185

				.reg .f32 %ssa_333;
				mul.f32 %ssa_333, %ssa_325, %ssa_186;	// vec1 32 ssa_333 = fmul ssa_325, ssa_186

				.reg .f32 %ssa_334_0;
				.reg .f32 %ssa_334_1;
				.reg .f32 %ssa_334_2;
				.reg .f32 %ssa_334_3;
				mov.f32 %ssa_334_0, %ssa_331;
				mov.f32 %ssa_334_1, %ssa_332;
				mov.f32 %ssa_334_2, %ssa_333; // vec3 32 ssa_334 = vec3 ssa_331, ssa_332, ssa_333

				.reg .f32 %ssa_335;
				add.f32 %ssa_335, %ssa_334_0, %ssa_334_1;
				add.f32 %ssa_335, %ssa_335, %ssa_334_2; // vec1 32 ssa_335 = fsum3 ssa_334

				.reg .f32 %ssa_336;
				mul.f32 %ssa_336, %ssa_335, %ssa_335;	// vec1 32 ssa_336 = fmul ssa_335, ssa_335

				.reg .f32 %ssa_337;
				neg.f32 %ssa_337, %ssa_336;	// vec1 32 ssa_337 = fneg ssa_336

				.reg .f32 %ssa_338;
				add.f32 %ssa_338, %ssa_337, %ssa_66;	// vec1 32 ssa_338 = fadd ssa_337, ssa_66

				.reg .f32 %ssa_339;
				mul.f32 %ssa_339, %ssa_327, %ssa_338;	// vec1 32 ssa_339 = fmul ssa_327, ssa_338

				.reg .f32 %ssa_340;
				mul.f32 %ssa_340, %ssa_327, %ssa_339;	// vec1 32 ssa_340 = fmul ssa_327, ssa_339

				.reg .f32 %ssa_341;
				neg.f32 %ssa_341, %ssa_340;	// vec1 32 ssa_341 = fneg ssa_340

				.reg .f32 %ssa_342;
				add.f32 %ssa_342, %ssa_341, %ssa_66;	// vec1 32 ssa_342 = fadd ssa_341, ssa_66

				.reg .f32 %ssa_343;
				sqrt.approx.f32 %ssa_343, %ssa_342;	// vec1 32 ssa_343 = fsqrt ssa_342

				.reg .f32 %ssa_344;
				mul.f32 %ssa_344, %ssa_327, %ssa_335;	// vec1 32 ssa_344 = fmul ssa_327, ssa_335

				.reg .f32 %ssa_345;
				add.f32 %ssa_345, %ssa_344, %ssa_343;	// vec1 32 ssa_345 = fadd ssa_344, ssa_343

				.reg .f32 %ssa_346;
				mul.f32 %ssa_346, %ssa_327, %ssa_184;	// vec1 32 ssa_346 = fmul ssa_327, ssa_184

				.reg .f32 %ssa_347;
				mul.f32 %ssa_347, %ssa_327, %ssa_185;	// vec1 32 ssa_347 = fmul ssa_327, ssa_185

				.reg .f32 %ssa_348;
				mul.f32 %ssa_348, %ssa_327, %ssa_186;	// vec1 32 ssa_348 = fmul ssa_327, ssa_186

				.reg .f32 %ssa_349;
				mul.f32 %ssa_349, %ssa_345, %ssa_323;	// vec1 32 ssa_349 = fmul ssa_345, ssa_323

				.reg .f32 %ssa_350;
				neg.f32 %ssa_350, %ssa_349;	// vec1 32 ssa_350 = fneg ssa_349

				.reg .f32 %ssa_351;
				mul.f32 %ssa_351, %ssa_345, %ssa_324;	// vec1 32 ssa_351 = fmul ssa_345, ssa_324

				.reg .f32 %ssa_352;
				neg.f32 %ssa_352, %ssa_351;	// vec1 32 ssa_352 = fneg ssa_351

				.reg .f32 %ssa_353;
				mul.f32 %ssa_353, %ssa_345, %ssa_325;	// vec1 32 ssa_353 = fmul ssa_345, ssa_325

				.reg .f32 %ssa_354;
				neg.f32 %ssa_354, %ssa_353;	// vec1 32 ssa_354 = fneg ssa_353

				.reg .f32 %ssa_355;
				add.f32 %ssa_355, %ssa_350, %ssa_346;	// vec1 32 ssa_355 = fadd ssa_350, ssa_346

				.reg .f32 %ssa_356;
				add.f32 %ssa_356, %ssa_352, %ssa_347;	// vec1 32 ssa_356 = fadd ssa_352, ssa_347

				.reg .f32 %ssa_357;
				add.f32 %ssa_357, %ssa_354, %ssa_348;	// vec1 32 ssa_357 = fadd ssa_354, ssa_348

				.reg .pred %ssa_358;
				setp.lt.f32 %ssa_358, %ssa_342, %ssa_1;	// vec1  1 ssa_358 = flt ssa_342, ssa_1

				.reg  .f32 %ssa_359;
				selp.f32 %ssa_359, %ssa_1_bits, %ssa_355, %ssa_358; // vec1 32 ssa_359 = bcsel ssa_358, ssa_1, ssa_355

				.reg  .f32 %ssa_360;
				selp.f32 %ssa_360, %ssa_1_bits, %ssa_356, %ssa_358; // vec1 32 ssa_360 = bcsel ssa_358, ssa_1, ssa_356

				.reg  .f32 %ssa_361;
				selp.f32 %ssa_361, %ssa_1_bits, %ssa_357, %ssa_358; // vec1 32 ssa_361 = bcsel ssa_358, ssa_1, ssa_357

				.reg .f32 %ssa_362;
				abs.f32 %ssa_362, %ssa_360;	// vec1 32 ssa_362 = fabs! ssa_360

				.reg .f32 %ssa_363;
				abs.f32 %ssa_363, %ssa_361;	// vec1 32 ssa_363 = fabs! ssa_361

				.reg .f32 %ssa_364;
				add.f32 %ssa_364, %ssa_362, %ssa_363;	// vec1 32 ssa_364 = fadd! ssa_362, ssa_363

				.reg .f32 %ssa_365;
				abs.f32 %ssa_365, %ssa_359;	// vec1 32 ssa_365 = fabs! ssa_359

				.reg .f32 %ssa_366;
				add.f32 %ssa_366, %ssa_365, %ssa_364;	// vec1 32 ssa_366 = fadd! ssa_365, ssa_364

				.reg .pred %ssa_367;
				setp.ne.f32 %ssa_367, %ssa_366, %ssa_1;	// vec1  1 ssa_367 = fneu! ssa_366, ssa_1

				// succs: block_22 block_23 
				// end_block block_21:
				//if
				@!%ssa_367 bra else_23;
				
					// start_block block_22:
					// preds: block_21 
					.reg .f32 %ssa_368;
	mov.f32 %ssa_368, 0F40a00000; // vec1 32 ssa_368 = load_const (0x40a00000 /* 5.000000 */)
					.reg .b32 %ssa_368_bits;
	mov.b32 %ssa_368_bits, 0F40a00000;

					.reg .f32 %ssa_369;
					neg.f32 %ssa_369, %ssa_38;	// vec1 32 ssa_369 = fneg ssa_38

					.reg .f32 %ssa_370;
					add.f32 %ssa_370, %ssa_66, %ssa_369;	// vec1 32 ssa_370 = fadd ssa_66, ssa_369

					.reg .f32 %ssa_371;
					add.f32 %ssa_371, %ssa_66, %ssa_38;	// vec1 32 ssa_371 = fadd ssa_66, ssa_38

					.reg .f32 %ssa_372;
					rcp.approx.f32 %ssa_372, %ssa_371;	// vec1 32 ssa_372 = frcp ssa_371

					.reg .f32 %ssa_373;
					mul.f32 %ssa_373, %ssa_370, %ssa_372;	// vec1 32 ssa_373 = fmul ssa_370, ssa_372

					.reg .f32 %ssa_374;
					mul.f32 %ssa_374, %ssa_373, %ssa_373;	// vec1 32 ssa_374 = fmul ssa_373, ssa_373

					.reg .f32 %ssa_375;
					neg.f32 %ssa_375, %ssa_374;	// vec1 32 ssa_375 = fneg ssa_374

					.reg .f32 %ssa_376;
					add.f32 %ssa_376, %ssa_66, %ssa_375;	// vec1 32 ssa_376 = fadd ssa_66, ssa_375

					.reg .f32 %ssa_377;
					neg.f32 %ssa_377, %ssa_330;	// vec1 32 ssa_377 = fneg ssa_330

					.reg .f32 %ssa_378;
					add.f32 %ssa_378, %ssa_66, %ssa_377;	// vec1 32 ssa_378 = fadd ssa_66, ssa_377

					.reg .f32 %ssa_379;
					lg2.approx.f32 %ssa_379, %ssa_378;
					mul.f32 %ssa_379, %ssa_379, %ssa_368;
					ex2.approx.f32 %ssa_379, %ssa_379;

					.reg .f32 %ssa_380;
					mul.f32 %ssa_380, %ssa_376, %ssa_379;	// vec1 32 ssa_380 = fmul ssa_376, ssa_379

					.reg .f32 %ssa_381;
					add.f32 %ssa_381, %ssa_374, %ssa_380;	// vec1 32 ssa_381 = fadd ssa_374, ssa_380

					mov.f32 %ssa_382, %ssa_381; // vec1 32 ssa_382 = phi block_22: ssa_381, block_23: ssa_66
					// succs: block_24 
					// end_block block_22:
					bra end_if_23;
				
				else_23: 
					// start_block block_23:
					// preds: block_21 
	mov.f32 %ssa_382, %ssa_66; // vec1 32 ssa_382 = phi block_22: ssa_381, block_23: ssa_66
					// succs: block_24 
					// end_block block_23:
				end_if_23:
				// start_block block_24:
				// preds: block_22 block_23 

				.reg .pred %ssa_383;
				setp.ge.s32 %ssa_383, %ssa_34, %ssa_1_bits; // vec1  1 ssa_383 = ige ssa_34, ssa_1

				// succs: block_25 block_26 
				// end_block block_24:
				//if
				@!%ssa_383 bra else_24;
				
					// start_block block_25:
					// preds: block_24 
					.reg .b64 %ssa_384;
	mov.b64 %ssa_384, %TextureSamplers; // vec1 32 ssa_384 = deref_var &TextureSamplers (uniform sampler2D[]) 

					.reg .b64 %ssa_385;
					.reg .u32 %ssa_385_array_index_32;
					.reg .u64 %ssa_385_array_index_64;
					cvt.u32.s32 %ssa_385_array_index_32, %ssa_34;
					mul.wide.u32 %ssa_385_array_index_64, %ssa_385_array_index_32, 32;
					add.u64 %ssa_385, %ssa_384, %ssa_385_array_index_64; // vec1 32 ssa_385 = deref_array &(*ssa_384)[ssa_34] (uniform sampler2D) /* &TextureSamplers[ssa_34] */

					.reg .f32 %ssa_386_0;
					.reg .f32 %ssa_386_1;
					.reg .f32 %ssa_386_2;
					.reg .f32 %ssa_386_3;
	txl %ssa_385, %ssa_385, %ssa_386_0, %ssa_386_1, %ssa_386_2, %ssa_386_3, %ssa_174_0, %ssa_174_1, %ssa_1; // vec4 32 ssa_386 = (float32)txl ssa_385 (texture_deref), ssa_385 (sampler_deref), ssa_174 (coord), ssa_1 (lod), texture non-uniform, sampler non-uniform

					.reg .f32 %ssa_453;
					mov.f32 %ssa_453, %ssa_386_0; // vec1 32 ssa_453 = mov ssa_386.x

					.reg .f32 %ssa_456;
					mov.f32 %ssa_456, %ssa_386_1; // vec1 32 ssa_456 = mov ssa_386.y

					.reg .f32 %ssa_459;
					mov.f32 %ssa_459, %ssa_386_2; // vec1 32 ssa_459 = mov ssa_386.z

					.reg .f32 %ssa_462;
					mov.f32 %ssa_462, %ssa_386_3; // vec1 32 ssa_462 = mov ssa_386.w

					mov.f32 %ssa_455, %ssa_453; // vec1 32 ssa_455 = phi block_25: ssa_453, block_26: ssa_552
					mov.f32 %ssa_458, %ssa_456; // vec1 32 ssa_458 = phi block_25: ssa_456, block_26: ssa_553
					mov.f32 %ssa_461, %ssa_459; // vec1 32 ssa_461 = phi block_25: ssa_459, block_26: ssa_554
					mov.f32 %ssa_464, %ssa_462; // vec1 32 ssa_464 = phi block_25: ssa_462, block_26: ssa_555
					// succs: block_27 
					// end_block block_25:
					bra end_if_24;
				
				else_24: 
					// start_block block_26:
					// preds: block_24 
					.reg .f32 %ssa_552;
	mov.f32 %ssa_552, 0F3f800000; // vec1 32 ssa_552 = load_const (0x3f800000 /* 1.000000 */)
					.reg .b32 %ssa_552_bits;
	mov.b32 %ssa_552_bits, 0F3f800000;

					.reg .f32 %ssa_553;
	mov.f32 %ssa_553, 0F3f800000; // vec1 32 ssa_553 = load_const (0x3f800000 /* 1.000000 */)
					.reg .b32 %ssa_553_bits;
	mov.b32 %ssa_553_bits, 0F3f800000;

					.reg .f32 %ssa_554;
	mov.f32 %ssa_554, 0F3f800000; // vec1 32 ssa_554 = load_const (0x3f800000 /* 1.000000 */)
					.reg .b32 %ssa_554_bits;
	mov.b32 %ssa_554_bits, 0F3f800000;

					.reg .f32 %ssa_555;
	mov.f32 %ssa_555, 0F3f800000; // vec1 32 ssa_555 = load_const (0x3f800000 /* 1.000000 */)
					.reg .b32 %ssa_555_bits;
	mov.b32 %ssa_555_bits, 0F3f800000;

					mov.f32 %ssa_455, %ssa_552; // vec1 32 ssa_455 = phi block_25: ssa_453, block_26: ssa_552
					mov.f32 %ssa_458, %ssa_553; // vec1 32 ssa_458 = phi block_25: ssa_456, block_26: ssa_553
					mov.f32 %ssa_461, %ssa_554; // vec1 32 ssa_461 = phi block_25: ssa_459, block_26: ssa_554
					mov.f32 %ssa_464, %ssa_555; // vec1 32 ssa_464 = phi block_25: ssa_462, block_26: ssa_555
					// succs: block_27 
					// end_block block_26:
				end_if_24:
				// start_block block_27:
				// preds: block_25 block_26 




				.reg .b32 %ssa_465_0;
				.reg .b32 %ssa_465_1;
				.reg .b32 %ssa_465_2;
				.reg .b32 %ssa_465_3;
				mov.b32 %ssa_465_0, %ssa_455;
				mov.b32 %ssa_465_1, %ssa_458;
				mov.b32 %ssa_465_2, %ssa_461;
				mov.b32 %ssa_465_3, %ssa_464; // vec4 32 ssa_465 = vec4 ssa_455, ssa_458, ssa_461, ssa_464

				.reg .f32 %ssa_388;
	mov.f32 %ssa_388, 0F00ffffff; // vec1 32 ssa_388 = load_const (0x00ffffff /* 0.000000 */)
				.reg .b32 %ssa_388_bits;
	mov.b32 %ssa_388_bits, 0F00ffffff;

				.reg .f32 %ssa_389;
	mov.f32 %ssa_389, 0F3c6ef35f; // vec1 32 ssa_389 = load_const (0x3c6ef35f /* 0.014584 */)
				.reg .b32 %ssa_389_bits;
	mov.b32 %ssa_389_bits, 0F3c6ef35f;

				.reg .f32 %ssa_390;
	mov.f32 %ssa_390, 0F0019660d; // vec1 32 ssa_390 = load_const (0x0019660d /* 0.000000 */)
				.reg .b32 %ssa_390_bits;
	mov.b32 %ssa_390_bits, 0F0019660d;

				.reg .s32 %ssa_391;
				mul.lo.s32 %ssa_391, %ssa_390_bits, %ssa_177; // vec1 32 ssa_391 = imul ssa_390, ssa_177

				.reg .s32 %ssa_392;
				add.s32 %ssa_392, %ssa_391, %ssa_389_bits; // vec1 32 ssa_392 = iadd ssa_391, ssa_389

				.reg .u32 %ssa_393;
				and.b32 %ssa_393, %ssa_392, %ssa_388;	// vec1 32 ssa_393 = iand ssa_392, ssa_388

				.reg .f32 %ssa_394;
				cvt.rn.f32.u32 %ssa_394, %ssa_393;	// vec1 32 ssa_394 = u2f32 ssa_393

				.reg .f32 %ssa_395;
	mov.f32 %ssa_395, 0F33800000; // vec1 32 ssa_395 = load_const (0x33800000 /* 0.000000 */)
				.reg .b32 %ssa_395_bits;
	mov.b32 %ssa_395_bits, 0F33800000;

				.reg .f32 %ssa_396;
				mul.f32 %ssa_396, %ssa_394, %ssa_395;	// vec1 32 ssa_396 = fmul ssa_394, ssa_395

				.reg .pred %ssa_397;
				setp.lt.f32 %ssa_397, %ssa_396, %ssa_382;	// vec1  1 ssa_397 = flt! ssa_396, ssa_382

				.reg .f32 %ssa_398;
	mov.f32 %ssa_398, 0F40000000; // vec1 32 ssa_398 = load_const (0x40000000 /* 2.000000 */)
				.reg .b32 %ssa_398_bits;
	mov.b32 %ssa_398_bits, 0F40000000;

				.reg .f32 %ssa_399;
				mul.f32 %ssa_399, %ssa_318, %ssa_398;	// vec1 32 ssa_399 = fmul ssa_318, ssa_398

				.reg .f32 %ssa_400;
				mul.f32 %ssa_400, %ssa_63, %ssa_399;	// vec1 32 ssa_400 = fmul ssa_63, ssa_399

				.reg .f32 %ssa_401;
				neg.f32 %ssa_401, %ssa_400;	// vec1 32 ssa_401 = fneg ssa_400

				.reg .f32 %ssa_402;
				mul.f32 %ssa_402, %ssa_64, %ssa_399;	// vec1 32 ssa_402 = fmul ssa_64, ssa_399

				.reg .f32 %ssa_403;
				neg.f32 %ssa_403, %ssa_402;	// vec1 32 ssa_403 = fneg ssa_402

				.reg .f32 %ssa_404;
				mul.f32 %ssa_404, %ssa_65, %ssa_399;	// vec1 32 ssa_404 = fmul ssa_65, ssa_399

				.reg .f32 %ssa_405;
				neg.f32 %ssa_405, %ssa_404;	// vec1 32 ssa_405 = fneg ssa_404

				.reg .f32 %ssa_406;
				add.f32 %ssa_406, %ssa_401, %ssa_184;	// vec1 32 ssa_406 = fadd ssa_401, ssa_184

				.reg .f32 %ssa_407;
				add.f32 %ssa_407, %ssa_403, %ssa_185;	// vec1 32 ssa_407 = fadd ssa_403, ssa_185

				.reg .f32 %ssa_408;
				add.f32 %ssa_408, %ssa_405, %ssa_186;	// vec1 32 ssa_408 = fadd ssa_405, ssa_186

				.reg  .f32 %ssa_409;
				selp.f32 %ssa_409, %ssa_406, %ssa_359, %ssa_397; // vec1 32 ssa_409 = bcsel ssa_397, ssa_406, ssa_359

				.reg  .f32 %ssa_410;
				selp.f32 %ssa_410, %ssa_407, %ssa_360, %ssa_397; // vec1 32 ssa_410 = bcsel ssa_397, ssa_407, ssa_360

				.reg  .f32 %ssa_411;
				selp.f32 %ssa_411, %ssa_408, %ssa_361, %ssa_397; // vec1 32 ssa_411 = bcsel ssa_397, ssa_408, ssa_361

				.reg .f32 %ssa_412_0;
				.reg .f32 %ssa_412_1;
				.reg .f32 %ssa_412_2;
				.reg .f32 %ssa_412_3;
				mov.f32 %ssa_412_0, %ssa_409;
				mov.f32 %ssa_412_1, %ssa_410;
				mov.f32 %ssa_412_2, %ssa_411;
				mov.f32 %ssa_412_3, %ssa_66; // vec4 32 ssa_412 = vec4 ssa_409, ssa_410, ssa_411, ssa_66

				.reg .b32 %ssa_413_0;
				.reg .b32 %ssa_413_1;
				.reg .b32 %ssa_413_2;
				.reg .b32 %ssa_413_3;
				mov.b32 %ssa_413_0, %ssa_465_0;
				mov.b32 %ssa_413_1, %ssa_465_1;
				mov.b32 %ssa_413_2, %ssa_465_2;
				mov.b32 %ssa_413_3, %ssa_48; // vec4 32 ssa_413 = vec4 ssa_465.x, ssa_465.y, ssa_465.z, ssa_48

				.reg .f32 %ssa_466;
				mov.f32 %ssa_466, %ssa_412_0; // vec1 32 ssa_466 = mov ssa_412.x

				.reg .f32 %ssa_469;
				mov.f32 %ssa_469, %ssa_412_1; // vec1 32 ssa_469 = mov ssa_412.y

				.reg .f32 %ssa_472;
				mov.f32 %ssa_472, %ssa_412_2; // vec1 32 ssa_472 = mov ssa_412.z

				.reg .f32 %ssa_475;
				mov.f32 %ssa_475, %ssa_412_3; // vec1 32 ssa_475 = mov ssa_412.w

				.reg .b32 %ssa_479;
				mov.b32 %ssa_479, %ssa_413_0; // vec1 32 ssa_479 = mov ssa_413.x

				.reg .b32 %ssa_482;
				mov.b32 %ssa_482, %ssa_413_1; // vec1 32 ssa_482 = mov ssa_413.y

				.reg .b32 %ssa_485;
				mov.b32 %ssa_485, %ssa_413_2; // vec1 32 ssa_485 = mov ssa_413.z

				.reg .b32 %ssa_488;
				mov.b32 %ssa_488, %ssa_413_3; // vec1 32 ssa_488 = mov ssa_413.w

				mov.s32 %ssa_416, %ssa_392; // vec1 32 ssa_416 = phi block_27: ssa_392, block_28: ssa_177
				mov.f32 %ssa_468, %ssa_466; // vec1 32 ssa_468 = phi block_27: ssa_466, block_28: ssa_556
				mov.f32 %ssa_471, %ssa_469; // vec1 32 ssa_471 = phi block_27: ssa_469, block_28: ssa_557
				mov.f32 %ssa_474, %ssa_472; // vec1 32 ssa_474 = phi block_27: ssa_472, block_28: ssa_558
				mov.f32 %ssa_477, %ssa_475; // vec1 32 ssa_477 = phi block_27: ssa_475, block_28: ssa_559
				mov.b32 %ssa_481, %ssa_479; // vec1 32 ssa_481 = phi block_27: ssa_479, block_28: ssa_480
				mov.b32 %ssa_484, %ssa_482; // vec1 32 ssa_484 = phi block_27: ssa_482, block_28: ssa_483
				mov.b32 %ssa_487, %ssa_485; // vec1 32 ssa_487 = phi block_27: ssa_485, block_28: ssa_486
				mov.b32 %ssa_490, %ssa_488; // vec1 32 ssa_490 = phi block_27: ssa_488, block_28: ssa_489
				// succs: block_29 
				// end_block block_27:
				bra end_if_22;
			
			else_22: 
				// start_block block_28:
				// preds: block_20 
				.reg .f32 %ssa_414_0;
				.reg .f32 %ssa_414_1;
				.reg .f32 %ssa_414_2;
				.reg .f32 %ssa_414_3;
	mov.f32 %ssa_414_0, 0F3f800000;
	mov.f32 %ssa_414_1, 0F00000000;
	mov.f32 %ssa_414_2, 0F00000000;
	mov.f32 %ssa_414_3, 0F00000000;
		// vec4 32 ssa_414 = load_const (0x3f800000 /* 1.000000 */, 0x00000000 /* 0.000000 */, 0x00000000 /* 0.000000 */, 0x00000000 /* 0.000000 */)

				.reg .f32 %ssa_415_0;
				.reg .f32 %ssa_415_1;
				.reg .f32 %ssa_415_2;
				.reg .f32 %ssa_415_3;
				mov.f32 %ssa_415_0, %ssa_32_0;
				mov.f32 %ssa_415_1, %ssa_32_1;
				mov.f32 %ssa_415_2, %ssa_32_2;
				mov.f32 %ssa_415_3, %ssa_48; // vec4 32 ssa_415 = vec4 ssa_32.x, ssa_32.y, ssa_32.z, ssa_48

				.reg .f32 %ssa_556;
	mov.f32 %ssa_556, 0F3f800000; // vec1 32 ssa_556 = load_const (0x3f800000 /* 1.000000 */)
				.reg .b32 %ssa_556_bits;
	mov.b32 %ssa_556_bits, 0F3f800000;

				.reg .f32 %ssa_557;
	mov.f32 %ssa_557, 0F00000000; // vec1 32 ssa_557 = load_const (0x00000000 /* 0.000000 */)
				.reg .b32 %ssa_557_bits;
	mov.b32 %ssa_557_bits, 0F00000000;

				.reg .f32 %ssa_558;
	mov.f32 %ssa_558, 0F00000000; // vec1 32 ssa_558 = load_const (0x00000000 /* 0.000000 */)
				.reg .b32 %ssa_558_bits;
	mov.b32 %ssa_558_bits, 0F00000000;

				.reg .f32 %ssa_559;
	mov.f32 %ssa_559, 0F00000000; // vec1 32 ssa_559 = load_const (0x00000000 /* 0.000000 */)
				.reg .b32 %ssa_559_bits;
	mov.b32 %ssa_559_bits, 0F00000000;

				.reg .f32 %ssa_480;
				mov.f32 %ssa_480, %ssa_415_0; // vec1 32 ssa_480 = mov ssa_415.x

				.reg .f32 %ssa_483;
				mov.f32 %ssa_483, %ssa_415_1; // vec1 32 ssa_483 = mov ssa_415.y

				.reg .f32 %ssa_486;
				mov.f32 %ssa_486, %ssa_415_2; // vec1 32 ssa_486 = mov ssa_415.z

				.reg .f32 %ssa_489;
				mov.f32 %ssa_489, %ssa_415_3; // vec1 32 ssa_489 = mov ssa_415.w

	mov.s32 %ssa_416, %ssa_177; // vec1 32 ssa_416 = phi block_27: ssa_392, block_28: ssa_177
				mov.f32 %ssa_468, %ssa_556; // vec1 32 ssa_468 = phi block_27: ssa_466, block_28: ssa_556
				mov.f32 %ssa_471, %ssa_557; // vec1 32 ssa_471 = phi block_27: ssa_469, block_28: ssa_557
				mov.f32 %ssa_474, %ssa_558; // vec1 32 ssa_474 = phi block_27: ssa_472, block_28: ssa_558
				mov.f32 %ssa_477, %ssa_559; // vec1 32 ssa_477 = phi block_27: ssa_475, block_28: ssa_559
				mov.b32 %ssa_481, %ssa_480; // vec1 32 ssa_481 = phi block_27: ssa_479, block_28: ssa_480
				mov.b32 %ssa_484, %ssa_483; // vec1 32 ssa_484 = phi block_27: ssa_482, block_28: ssa_483
				mov.b32 %ssa_487, %ssa_486; // vec1 32 ssa_487 = phi block_27: ssa_485, block_28: ssa_486
				mov.b32 %ssa_490, %ssa_489; // vec1 32 ssa_490 = phi block_27: ssa_488, block_28: ssa_489
				// succs: block_29 
				// end_block block_28:
			end_if_22:
			// start_block block_29:
			// preds: block_27 block_28 









			.reg .b32 %ssa_491_0;
			.reg .b32 %ssa_491_1;
			.reg .b32 %ssa_491_2;
			.reg .b32 %ssa_491_3;
			mov.b32 %ssa_491_0, %ssa_481;
			mov.b32 %ssa_491_1, %ssa_484;
			mov.b32 %ssa_491_2, %ssa_487;
			mov.b32 %ssa_491_3, %ssa_490; // vec4 32 ssa_491 = vec4 ssa_481, ssa_484, ssa_487, ssa_490

			.reg .b32 %ssa_478_0;
			.reg .b32 %ssa_478_1;
			.reg .b32 %ssa_478_2;
			.reg .b32 %ssa_478_3;
			mov.b32 %ssa_478_0, %ssa_468;
			mov.b32 %ssa_478_1, %ssa_471;
			mov.b32 %ssa_478_2, %ssa_474;
			mov.b32 %ssa_478_3, %ssa_477; // vec4 32 ssa_478 = vec4 ssa_468, ssa_471, ssa_474, ssa_477

			.reg .b32 %ssa_493;
			mov.b32 %ssa_493, %ssa_478_0; // vec1 32 ssa_493 = mov ssa_478.x

			.reg .b32 %ssa_496;
			mov.b32 %ssa_496, %ssa_478_1; // vec1 32 ssa_496 = mov ssa_478.y

			.reg .b32 %ssa_499;
			mov.b32 %ssa_499, %ssa_478_2; // vec1 32 ssa_499 = mov ssa_478.z

			.reg .b32 %ssa_502;
			mov.b32 %ssa_502, %ssa_478_3; // vec1 32 ssa_502 = mov ssa_478.w

			.reg .b32 %ssa_506;
			mov.b32 %ssa_506, %ssa_491_0; // vec1 32 ssa_506 = mov ssa_491.x

			.reg .b32 %ssa_509;
			mov.b32 %ssa_509, %ssa_491_1; // vec1 32 ssa_509 = mov ssa_491.y

			.reg .b32 %ssa_512;
			mov.b32 %ssa_512, %ssa_491_2; // vec1 32 ssa_512 = mov ssa_491.z

			.reg .b32 %ssa_515;
			mov.b32 %ssa_515, %ssa_491_3; // vec1 32 ssa_515 = mov ssa_491.w

			mov.s32 %ssa_419, %ssa_416; // vec1 32 ssa_419 = phi block_19: ssa_285, block_29: ssa_416
			mov.b32 %ssa_494, %ssa_493; // vec1 32 ssa_494 = phi block_19: ssa_492, block_29: ssa_493
			mov.b32 %ssa_497, %ssa_496; // vec1 32 ssa_497 = phi block_19: ssa_495, block_29: ssa_496
			mov.b32 %ssa_500, %ssa_499; // vec1 32 ssa_500 = phi block_19: ssa_498, block_29: ssa_499
			mov.b32 %ssa_503, %ssa_502; // vec1 32 ssa_503 = phi block_19: ssa_501, block_29: ssa_502
			mov.b32 %ssa_507, %ssa_506; // vec1 32 ssa_507 = phi block_19: ssa_505, block_29: ssa_506
			mov.b32 %ssa_510, %ssa_509; // vec1 32 ssa_510 = phi block_19: ssa_508, block_29: ssa_509
			mov.b32 %ssa_513, %ssa_512; // vec1 32 ssa_513 = phi block_19: ssa_511, block_29: ssa_512
			mov.b32 %ssa_516, %ssa_515; // vec1 32 ssa_516 = phi block_19: ssa_514, block_29: ssa_515
			// succs: block_30 
			// end_block block_29:
		end_if_19:
		// start_block block_30:
		// preds: block_19 block_29 









		.reg .b32 %ssa_517_0;
		.reg .b32 %ssa_517_1;
		.reg .b32 %ssa_517_2;
		.reg .b32 %ssa_517_3;
		mov.b32 %ssa_517_0, %ssa_507;
		mov.b32 %ssa_517_1, %ssa_510;
		mov.b32 %ssa_517_2, %ssa_513;
		mov.b32 %ssa_517_3, %ssa_516; // vec4 32 ssa_517 = vec4 ssa_507, ssa_510, ssa_513, ssa_516

		.reg .b32 %ssa_504_0;
		.reg .b32 %ssa_504_1;
		.reg .b32 %ssa_504_2;
		.reg .b32 %ssa_504_3;
		mov.b32 %ssa_504_0, %ssa_494;
		mov.b32 %ssa_504_1, %ssa_497;
		mov.b32 %ssa_504_2, %ssa_500;
		mov.b32 %ssa_504_3, %ssa_503; // vec4 32 ssa_504 = vec4 ssa_494, ssa_497, ssa_500, ssa_503

		.reg .b32 %ssa_519;
		mov.b32 %ssa_519, %ssa_504_0; // vec1 32 ssa_519 = mov ssa_504.x

		.reg .b32 %ssa_522;
		mov.b32 %ssa_522, %ssa_504_1; // vec1 32 ssa_522 = mov ssa_504.y

		.reg .b32 %ssa_525;
		mov.b32 %ssa_525, %ssa_504_2; // vec1 32 ssa_525 = mov ssa_504.z

		.reg .b32 %ssa_528;
		mov.b32 %ssa_528, %ssa_504_3; // vec1 32 ssa_528 = mov ssa_504.w

		.reg .b32 %ssa_532;
		mov.b32 %ssa_532, %ssa_517_0; // vec1 32 ssa_532 = mov ssa_517.x

		.reg .b32 %ssa_535;
		mov.b32 %ssa_535, %ssa_517_1; // vec1 32 ssa_535 = mov ssa_517.y

		.reg .b32 %ssa_538;
		mov.b32 %ssa_538, %ssa_517_2; // vec1 32 ssa_538 = mov ssa_517.z

		.reg .b32 %ssa_541;
		mov.b32 %ssa_541, %ssa_517_3; // vec1 32 ssa_541 = mov ssa_517.w

		mov.s32 %ssa_422, %ssa_419; // vec1 32 ssa_422 = phi block_9: ssa_216, block_30: ssa_419
		mov.b32 %ssa_520, %ssa_519; // vec1 32 ssa_520 = phi block_9: ssa_518, block_30: ssa_519
		mov.b32 %ssa_523, %ssa_522; // vec1 32 ssa_523 = phi block_9: ssa_521, block_30: ssa_522
		mov.b32 %ssa_526, %ssa_525; // vec1 32 ssa_526 = phi block_9: ssa_524, block_30: ssa_525
		mov.b32 %ssa_529, %ssa_528; // vec1 32 ssa_529 = phi block_9: ssa_527, block_30: ssa_528
		mov.b32 %ssa_533, %ssa_532; // vec1 32 ssa_533 = phi block_9: ssa_531, block_30: ssa_532
		mov.b32 %ssa_536, %ssa_535; // vec1 32 ssa_536 = phi block_9: ssa_534, block_30: ssa_535
		mov.b32 %ssa_539, %ssa_538; // vec1 32 ssa_539 = phi block_9: ssa_537, block_30: ssa_538
		mov.b32 %ssa_542, %ssa_541; // vec1 32 ssa_542 = phi block_9: ssa_540, block_30: ssa_541
		// succs: block_31 
		// end_block block_30:
	end_if_16:
	// start_block block_31:
	// preds: block_9 block_30 









	.reg .b32 %ssa_543_0;
	.reg .b32 %ssa_543_1;
	.reg .b32 %ssa_543_2;
	.reg .b32 %ssa_543_3;
	mov.b32 %ssa_543_0, %ssa_533;
	mov.b32 %ssa_543_1, %ssa_536;
	mov.b32 %ssa_543_2, %ssa_539;
	mov.b32 %ssa_543_3, %ssa_542; // vec4 32 ssa_543 = vec4 ssa_533, ssa_536, ssa_539, ssa_542

	.reg .b32 %ssa_530_0;
	.reg .b32 %ssa_530_1;
	.reg .b32 %ssa_530_2;
	.reg .b32 %ssa_530_3;
	mov.b32 %ssa_530_0, %ssa_520;
	mov.b32 %ssa_530_1, %ssa_523;
	mov.b32 %ssa_530_2, %ssa_526;
	mov.b32 %ssa_530_3, %ssa_529; // vec4 32 ssa_530 = vec4 ssa_520, ssa_523, ssa_526, ssa_529

	st.global.b32 [%ssa_176], %ssa_422; // intrinsic store_deref (%ssa_176, %ssa_422) (1, 0) /* wrmask=x */ /* access=0 */

	.reg .b64 %ssa_425;
	add.u64 %ssa_425, %ssa_175, 0; // vec1 32 ssa_425 = deref_struct &ssa_175->ColorAndDistance (shader_call_data vec4) /* &Ray.ColorAndDistance */

	st.global.b32 [%ssa_425 + 0], %ssa_543_0;
	st.global.b32 [%ssa_425 + 4], %ssa_543_1;
	st.global.b32 [%ssa_425 + 8], %ssa_543_2;
	st.global.b32 [%ssa_425 + 12], %ssa_543_3;
// intrinsic store_deref (%ssa_425, %ssa_543) (15, 0) /* wrmask=xyzw */ /* access=0 */


	.reg .b64 %ssa_426;
	add.u64 %ssa_426, %ssa_175, 16; // vec1 32 ssa_426 = deref_struct &ssa_175->ScatterDirection (shader_call_data vec4) /* &Ray.ScatterDirection */

	st.global.b32 [%ssa_426 + 0], %ssa_530_0;
	st.global.b32 [%ssa_426 + 4], %ssa_530_1;
	st.global.b32 [%ssa_426 + 8], %ssa_530_2;
	st.global.b32 [%ssa_426 + 12], %ssa_530_3;
// intrinsic store_deref (%ssa_426, %ssa_530) (15, 0) /* wrmask=xyzw */ /* access=0 */


	// succs: block_32 
	// end_block block_31:
	// block block_32:
	shader_exit:
	ret ;
}
